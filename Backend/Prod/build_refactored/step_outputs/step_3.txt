Step: step_3
Description: Créer test unitaire Backend/Prod/tests/test_orchestrator_fixes.py : (1) Test race condition : créer plan JSON avec 3 steps modifiant le même fichier, exécuter avec orchestrator, vérifier exécution séquentielle forcée et intégrité du fichier final. (2) Test surgical mode : exécuter plan en mode FAST, vérifier surgical_mode=False dans les logs, exécuter même plan en mode BUILD, vérifier surgical_mode=True. Utiliser pytest et fixtures appropriées.
Type: code_generation
Success: True
Tokens: 74746
Cost: $0.0116
Time: 314267ms

============================================================

I'll create a comprehensive test suite for race condition prevention in the orchestrator, following TDD principles and the refactoring guidelines. Let me structure this properly:

```python
"""
Test suite for race condition prevention in Orchestrator.
Tests that steps modifying the same file execute sequentially and maintain file integrity.
"""

import json
import asyncio
import tempfile
import time
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple
import pytest
from unittest.mock import Mock, AsyncMock, patch

# Models
from ..core.models import Plan, Step, StepResult
from ..core.orchestrator import Orchestrator
from ..core.agent_router import AgentRouter
from ..core.metrics import MetricsCollector


# ============================================================================
# MODELS
# ============================================================================

class TestFile:
    """Represents a test file for race condition testing."""
    
    def __init__(self, content: str = "", suffix: str = ".py"):
        """Initialize a test file.
        
        Args:
            content: Initial file content
            suffix: File extension
        """
        self.content = content
        self.suffix = suffix
        self.path: Optional[Path] = None
        self._temp_file = None
    
    def __enter__(self) -> 'TestFile':
        """Context manager entry."""
        self._temp_file = tempfile.NamedTemporaryFile(
            mode='w', 
            suffix=self.suffix, 
            delete=False,
            encoding='utf-8'
        )
        self._temp_file.write(self.content)
        self._temp_file.close()
        self.path = Path(self._temp_file.name)
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit - cleanup."""
        if self.path and self.path.exists():
            self.path.unlink()
    
    def read(self) -> str:
        """Read file content."""
        if self.path:
            return self.path.read_text(encoding='utf-8')
        return ""
    
    def write(self, content: str):
        """Write content to file."""
        if self.path:
            self.path.write_text(content, encoding='utf-8')


class ExecutionTracker:
    """Tracks execution order and concurrency for testing."""
    
    def __init__(self):
        """Initialize execution tracker."""
        self.execution_order: List[str] = []
        self.active_steps: Set[str] = set()
        self.max_concurrent: int = 0
        self.execution_intervals: Dict[str, Tuple[float, float]] = {}
    
    def start_step(self, step_id: str) -> None:
        """Record step start.
        
        Args:
            step_id: Step identifier
        """
        self.execution_order.append(step_id)
        self.active_steps.add(step_id)
        self.max_concurrent = max(self.max_concurrent, len(self.active_steps))
        self.execution_intervals[step_id] = (time.time(), 0)
    
    def end_step(self, step_id: str) -> None:
        """Record step end.
        
        Args:
            step_id: Step identifier
        """
        self.active_steps.discard(step_id)
        start_time, _ = self.execution_intervals[step_id]
        self.execution_intervals[step_id] = (start_time, time.time())


# ============================================================================
# SERVICES
# ============================================================================

class MockAgentRouterService:
    """Service for mocking AgentRouter behavior in tests."""
    
    def __init__(self):
        """Initialize mock service."""
        self.execution_order: List[str] = []
        self.file_modifications: Dict[str, List[str]] = {}
        self.should_fail_steps: Set[str] = set()
        self.execution_delay: float = 0.1
    
    def create_mock_router(self, mocker) -> Mock:
        """Create a mock AgentRouter.
        
        Args:
            mocker: Pytest mocker fixture
            
        Returns:
            Mock AgentRouter instance
        """
        router = mocker.MagicMock(spec=AgentRouter)
        
        async def mock_execute_step(
            step: Step, 
            context: str, 
            surgical_mode: bool = False, 
            loaded_files: Optional[Dict] = None
        ) -> StepResult:
            """Mock step execution with tracking."""
            
            # Simulate processing delay
            await asyncio.sleep(self.execution_delay)
            
            # Track execution order
            self.execution_order.append(step.id)
            
            # Track file modifications
            if loaded_files:
                for file_path in loaded_files:
                    if file_path not in self.file_modifications:
                        self.file_modifications[file_path] = []
                    self.file_modifications[file_path].append(step.id)
            
            # Check if step should fail
            if step.id in self.should_fail_steps:
                return StepResult(
                    step_id=step.id,
                    success=False,
                    output="Simulated failure",
                    execution_time=self.execution_delay,
                    token_usage={"input": 100, "output": 0}
                )
            
            # Generate surgical instructions based on step
            output = self._generate_surgical_output(step)
            
            return StepResult(
                step_id=step.id,
                success=True,
                output=output,
                execution_time=self.execution_delay,
                token_usage={"input": 100, "output": 50}
            )
        
        router.execute_step = mock_execute_step
        return router
    
    def _generate_surgical_output(self, step: Step) -> str:
        """Generate surgical JSON output for a step.
        
        Args:
            step: Step to generate output for
            
        Returns:
            JSON string with surgical operations
        """
        if step.id.startswith("step_"):
            step_num = step.id.split("_")[1]
            return json.dumps({
                "operations": [{
                    "type": "add_method",
                    "target": "TestClass",
                    "position": "end",
                    "code": f"    def method_{step_num}(self):\n        return 'added by {step.id}'"
                }]
            })
        elif step.id.startswith("step"):
            # Handle stepA, stepB, stepC format
            step_letter = step.id.replace("step", "")
            return json.dumps({
                "operations": [{
                    "type": "add_method",
                    "target": "TestClass",
                    "position": "end",
                    "code": f"    def method_{step_letter.lower()}(self):\n        return 'added by {step.id}'"
                }]
            })
        return "{}"
    
    def reset(self) -> None:
        """Reset tracking data."""
        self.execution_order.clear()
        self.file_modifications.clear()
        self.should_fail_steps.clear()


class PlanGeneratorService:
    """Service for generating test plans."""
    
    @staticmethod
    def create_race_condition_plan(
        test_file_path: Path,
        num_steps: int = 3,
        step_prefix: str = "step_"
    ) -> Plan:
        """Create a plan with multiple steps modifying the same file.
        
        Args:
            test_file_path: Path to the file all steps will modify
            num_steps: Number of steps in the plan
            step_prefix: Prefix for step IDs
            
        Returns:
            Plan with steps targeting the same file
        """
        steps = []
        for i in range(1, num_steps + 1):
            step_id = f"{step_prefix}{i}"
            steps.append(
                Step(
                    id=step_id,
                    name=f"Add method {i}",
                    description=f"Add method_{i} to TestClass",
                    type="code_generation",
                    context={"surgical_mode": True},
                    input_files={},
                    output_files=[str(test_file_path)],
                    dependencies=[],  # No explicit dependencies
                    agent_preference=None
                )
            )
        
        return Plan(
            task_id="race_condition_test",
            name="Race Condition Test",
            description="Test sequential execution when multiple steps modify the same file",
            steps=steps
        )
    
    @staticmethod
    def create_dependent_steps_plan(
        test_file_path: Path,
        step_ids: List[str] = None
    ) -> Plan:
        """Create a plan with explicit step dependencies.
        
        Args:
            test_file_path: Path to the file all steps will modify
            step_ids: List of step IDs (default: ["step_a", "step_b", "step_c"])
            
        Returns:
            Plan with dependent steps
        """
        if step_ids is None:
            step_ids = ["step_a", "step_b", "step_c"]
        
        steps = []
        for i, step_id in enumerate(step_ids):
            dependencies = []
            if i > 0:
                dependencies.append(step_ids[i - 1])
            
            steps.append(
                Step(
                    id=step_id,
                    name=f"Step {step_id}",
                    description=f"Step {step_id} with dependencies {dependencies}",
                    type="code_generation",
                    context={"surgical_mode": True},
                    input_files={},
                    output_files=[str(test_file_path)],
                    dependencies=dependencies,
                    agent_preference=None
                )
            )
        
        return Plan(
            task_id="dependent_steps_test",
            name="Dependent Steps Test",
            description="Test execution with explicit dependencies",
            steps=steps
        )
    
    @staticmethod
    def plan_to_json(plan: Plan) -> Dict:
        """Convert plan to JSON-serializable dictionary.
        
        Args:
            plan: Plan to convert
            
        Returns:
            Dictionary representation of the plan
        """
        return {
            "task_id": plan.task_id,
            "name": plan.name,
            "description": plan.description,
            "steps": [
                {
                    "id": step.id,
                    "name": step.name,
                    "description": step.description,
                    "type": step.type,
                    "context": step.context,
                    "input_files": step.input_files,
                    "output_files": step.output_files,
                    "dependencies": step.dependencies,
                    "agent_preference": step.agent_preference
                }
                for step in plan.steps
            ]
        }


class FileIntegrityValidator:
    """Service for validating file integrity during tests."""
    
    @staticmethod
    def validate_file_content(
        file_path: Path,
        expected_methods: List[str],
        original_methods: List[str] = None
    ) -> Tuple[bool, List[str]]:
        """Validate file content after execution.
        
        Args:
            file_path: Path to the file to validate
            expected_methods: List of method names that should be present
            original_methods: List of original method names that should be preserved
            
        Returns:
            Tuple of (is_valid, error_messages)
        """
        errors = []
        
        if not file_path.exists():
            errors.append(f"File does not exist: {file_path}")
            return False, errors
        
        try:
            content = file_path.read_text(encoding='utf-8')
        except Exception as e:
            errors.append(f"Failed to read file: {e}")
            return False, errors
        
        # Check expected methods
        for method in expected_methods:
            if f"def {method}(" not in content:
                errors.append(f"Expected method '{method}' not found in file")
        
        # Check original methods (if provided)
        if original_methods:
            for method in original_methods:
                if f"def {method}(" not in content:
                    errors.append(f"Original method '{method}' was lost")
        
        # Check for duplicates
        for method in expected_methods:
            count = content.count(f"def {method}(")
            if count > 1:
                errors.append(f"Method '{method}' appears {count} times (should be 1)")
        
        # Check Python syntax
        try:
            import ast
            ast.parse(content)
        except SyntaxError as e:
            errors.append(f"File contains invalid Python syntax: {e}")
        
        return len(errors) == 0, errors
    
    @staticmethod
    def validate_no_overlapping_execution(
        execution_intervals: Dict[str, Tuple[float, float]]
    ) -> Tuple[bool, List[str]]:
        """Validate that execution intervals don't overlap.
        
        Args:
            execution_intervals: Dictionary mapping step_id to (start_time, end_time)
            
        Returns:
            Tuple of (no_overlap, error_messages)
        """
        errors = []
        intervals = list(execution_intervals.values())
        
        for i in range(len(intervals)):
            for j in range(i + 1, len(intervals)):
                start_i, end_i = intervals[i]
                start_j, end_j = intervals[j]
                
                # Check for overlap
                if not (end_i <= start_j or end_j <= start_i):
                    errors.append(
                        f"Overlap detected between intervals {i} and {j}: "
                        f"{start_i}-{end_i} overlaps with {start_j}-{end_j}"
                    )
        
        return len(errors) == 0, errors


# ============================================================================
# TEST CONTROLLERS
# ============================================================================

class RaceConditionTestController:
    """Controller for race condition tests."""
    
    def __init__(self):
        """Initialize test controller."""
        self.mock_service = MockAgentRouterService()
        self.plan_generator = PlanGeneratorService()
        self.validator = FileIntegrityValidator()
    
    async def run_sequential_execution_test(
        self,
        orchestrator: Orchestrator,
        test_file: TestFile,
        num_steps: int = 3
    ) -> Tuple[bool, Dict]:
        """Run sequential execution test.
        
        Args:
            orchestrator: Orchestrator instance
            test_file: Test file to modify
            num_steps: Number of steps to execute
            
        Returns:
            Tuple of (test_passed, results_dict)
        """
        # Create plan
        plan = self.plan_generator.create_race_condition_plan(
            test_file.path,
            num_steps=num_steps
        )
        
        # Execute plan
        results = await orchestrator.execute_plan(plan)
        
        # Verify results
        all_succeeded = all(result.success for result in results.values())
        
        # Verify execution order
        execution_order_correct = (
            self.mock_service.execution_order == [f"step_{i}" for i in range(1, num_steps + 1)]
        )
        
        # Verify file modifications
        file_was_modified = str(test_file.path) in self.mock_service.file_modifications
        if file_was_modified:
            modification_order = self.mock_service.file_modifications[str(test_file.path)]
            modification_order_correct = modification_order == [f"step_{i}" for i in range(1, num_steps + 1)]
        else:
            modification_order_correct = False
        
        # Verify file content
        expected_methods = [f"method_{i}" for i in range(1, num_steps + 1)]
        original_methods = ["get_value", "increment", "helper_function"]
        
        file_valid, file_errors = self.validator.validate_file_content(
            test_file.path,
            expected_methods,
            original_methods
        )
        
        test_passed = (
            all_succeeded and
            execution_order_correct and
            file_was_modified and
            modification_order_correct and
            file_valid
        )
        
        return test_passed, {
            "all_succeeded": all_succeeded,
            "execution_order": self.mock_service.execution_order.copy(),
            "execution_order_correct": execution_order_correct,
            "file_was_modified": file_was_modified,
            "modification_order": self.mock_service.file_modifications.get(str(test_file.path), []),
            "modification_order_correct": modification_order_correct,
            "file_valid": file_valid,
            "file_errors": file_errors,
            "results": {k: v.success for k, v in results.items()}
        }
    
    async def run_error_recovery_test(
        self,
        orchestrator: Orchestrator,
        test_file: TestFile,
        failing_step: str = "step_2"
    ) -> Tuple[bool, Dict]:
        """Run error recovery test.
        
        Args:
            orchestrator: Orchestrator instance
            test_file: Test file to modify
            failing_step: Step ID that should fail
            
        Returns:
            Tuple of (test_passed, results_dict)
        """
        # Configure mock to fail specific step
        self.mock_service.should_fail_steps.add(failing_step)
        
        # Create plan
        plan = self.plan_generator.create_race_condition_plan(
            test_file.path,
            num_steps=3
        )
        
        # Execute plan
        results = await orchestrator.execute_plan(plan)
        
        # Verify step_2 failed
        step_2_failed = not results["step_2"].success
        step_1_succeeded = results["step_1"].success
        
        # Verify file contains only successful modifications
        content = test_file.read()
        has_step_1_method = "def method_1(" in content
        has_step_2_method = "def method_2(" in content
        has_step_3_method = "def method_3(" in content
        
        # File should still be valid Python
        import ast
        try:
            ast.parse(content)
            syntax_valid = True
        except SyntaxError:
            syntax_valid = False
        
        test_passed = (
            step_2_failed and
            step_1_succeeded and
            has_step_1_method and
            not has_step_2_method and
            syntax_valid
        )
        
        return test_passed, {
            "step_2_failed": step_2_failed,
            "step_1_succeeded": step_1_succeeded,
            "has_step_1_method": has_step_1_method,
            "has_step_2_method": has_step_2_method,
            "has_step_3_method": has_step_3_method,
            "syntax_valid": syntax_valid,
            "results": {k: v.success for k, v in results.items()}
        }


# ============================================================================
# TEST SUITES
# ============================================================================

class TestRaceConditionPrevention:
    """Test suite for race condition prevention in Orchestrator."""
    
    @pytest.fixture
    def temp_test_file(self):
        """Create a temporary Python file for testing."""
        with TestFile("""# Test file for race condition testing
import time

class TestClass:
    def __init__(self):
        self.value = 0
    
    def get_value(self):
        return self.value
    
    def increment(self):
        self.value += 1
        return self.value

def helper_function():
    return

I'll create a comprehensive test suite for surgical mode execution based on the requirements. Let me refactor the draft code following TDD principles and SOLID design.

```python
"""
Test suite for surgical mode execution in Orchestrator.
Tests that surgical mode is correctly enabled/disabled based on execution mode.
"""

import pytest
import asyncio
import tempfile
import shutil
import json
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any
from unittest.mock import Mock, patch, AsyncMock, MagicMock
import sys

# Add the project root to the path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from Backend.Prod.core.orchestrator import Orchestrator
from Backend.Prod.core.models import Plan, Step, StepResult
from Backend.Prod.core.agent_router import AgentRouter
from Backend.Prod.core.metrics import MetricsCollector


# ============================================================================
# MODELS
# ============================================================================

class TestPlanFactory:
    """Factory for creating test plans with different configurations."""
    
    @staticmethod
    def create_surgical_mode_test_plan(
        project_dir: Path,
        step_id: str = "step_1",
        step_type: str = "code_generation",
        surgical_mode_in_context: Optional[bool] = True,
        existing_files: Optional[List[str]] = None
    ) -> Plan:
        """
        Create a test plan for surgical mode testing.
        
        Args:
            project_dir: Project directory path
            step_id: Step identifier
            step_type: Type of step (code_generation, refactoring, etc.)
            surgical_mode_in_context: Whether to enable surgical mode in context
            existing_files: List of existing files to include in context
            
        Returns:
            Plan object configured for testing
        """
        if existing_files is None:
            existing_files = [
                str(project_dir / "test_module.py"),
                str(project_dir / "another_module.py")
            ]
        
        context = {"existing_files": existing_files}
        if surgical_mode_in_context is not None:
            context["surgical_mode"] = surgical_mode_in_context
        
        return Plan(
            task_id="test_surgical_mode",
            name="Surgical Mode Test",
            description="Test surgical mode activation based on execution mode",
            steps=[
                Step(
                    id=step_id,
                    name=f"Test {step_type} step",
                    description=f"Test {step_type} with surgical mode",
                    type=step_type,
                    context=context,
                    input_files={},
                    output_files=existing_files,
                    dependencies=[],
                    agent_preference=None
                )
            ]
        )
    
    @staticmethod
    def create_multi_step_plan(
        project_dir: Path,
        num_steps: int = 3,
        step_type: str = "code_generation",
        surgical_mode_in_context: Optional[bool] = True
    ) -> Plan:
        """
        Create a multi-step test plan.
        
        Args:
            project_dir: Project directory path
            num_steps: Number of steps to create
            step_type: Type of steps
            surgical_mode_in_context: Whether to enable surgical mode in context
            
        Returns:
            Plan with multiple steps
        """
        steps = []
        for i in range(num_steps):
            context = {
                "existing_files": [str(project_dir / f"test_module_{i}.py")],
                "surgical_mode": surgical_mode_in_context
            }
            
            steps.append(
                Step(
                    id=f"step_{i+1}",
                    name=f"Step {i+1}",
                    description=f"Test step {i+1}",
                    type=step_type,
                    context=context,
                    input_files={},
                    output_files=[str(project_dir / f"test_module_{i}.py")],
                    dependencies=[] if i == 0 else [f"step_{i}"],
                    agent_preference=None
                )
            )
        
        return Plan(
            task_id="multi_step_test",
            name="Multi-Step Test",
            description="Test multiple steps with surgical mode",
            steps=steps
        )


# ============================================================================
# SERVICES
# ============================================================================

class MockAgentRouterService:
    """Service for creating mock agent routers with different behaviors."""
    
    @staticmethod
    def create_successful_router() -> Mock:
        """
        Create a mock agent router that always succeeds.
        
        Returns:
            Mock AgentRouter configured for success
        """
        router = Mock(spec=AgentRouter)
        router.execute_step = AsyncMock(return_value=Mock(
            success=True,
            output=json.dumps({
                "operations": [{
                    "type": "add_method",
                    "target": "TestClass",
                    "position": "end",
                    "code": "def new_method(self):\n    return \"new method added\""
                }]
            }),
            execution_time=0.1,
            token_usage={"input": 100, "output": 50}
        ))
        return router
    
    @staticmethod
    def create_failing_router(step_id_to_fail: str = "step_2") -> Mock:
        """
        Create a mock agent router that fails for specific steps.
        
        Args:
            step_id_to_fail: Step ID that should fail
            
        Returns:
            Mock AgentRouter configured to fail for specific steps
        """
        router = Mock(spec=AgentRouter)
        
        async def execute_step(step, context, surgical_mode=False, loaded_files=None):
            if step.id == step_id_to_fail:
                return StepResult(
                    step_id=step.id,
                    success=False,
                    output="Simulated failure",
                    execution_time=0.1,
                    token_usage={"input": 100, "output": 0}
                )
            
            return StepResult(
                step_id=step.id,
                success=True,
                output=json.dumps({
                    "operations": [{
                        "type": "add_method",
                        "target": "TestClass",
                        "position": "end",
                        "code": f"def method_{step.id}(self):\n    return \"{step.id}\""
                    }]
                }),
                execution_time=0.1,
                token_usage={"input": 100, "output": 50}
            )
        
        router.execute_step = execute_step
        return router
    
    @staticmethod
    def create_tracking_router() -> Mock:
        """
        Create a mock agent router that tracks execution.
        
        Returns:
            Mock AgentRouter that tracks execution order and parameters
        """
        router = Mock(spec=AgentRouter)
        router.execution_order = []
        router.surgical_mode_calls = []
        
        async def execute_step(step, context, surgical_mode=False, loaded_files=None):
            router.execution_order.append(step.id)
            router.surgical_mode_calls.append(surgical_mode)
            
            await asyncio.sleep(0.01)  # Simulate processing
            
            return StepResult(
                step_id=step.id,
                success=True,
                output=json.dumps({
                    "operations": [{
                        "type": "add_method",
                        "target": "TestClass",
                        "position": "end",
                        "code": f"def method_{step.id}(self):\n    return \"{step.id}\""
                    }]
                }),
                execution_time=0.1,
                token_usage={"input": 100, "output": 50}
            )
        
        router.execute_step = execute_step
        return router


class MockSurgicalEditorService:
    """Service for mocking surgical editor components."""
    
    @staticmethod
    def create_mock_editor() -> Mock:
        """
        Create a mock SurgicalEditor.
        
        Returns:
            Mock SurgicalEditor
        """
        with patch('Backend.Prod.core.orchestrator.SurgicalEditor') as mock_editor_class:
            editor_instance = Mock()
            editor_instance.prepare.return_value = True
            editor_instance.get_ast_context.return_value = "AST context for test file"
            editor_instance.apply_instructions.return_value = (True, "modified code", "original code")
            
            mock_editor_class.return_value = editor_instance
            mock_editor_class.create_new_file.return_value = (True, "File created successfully")
            
            return mock_editor_class
    
    @staticmethod
    def create_mock_parser() -> Mock:
        """
        Create a mock SurgicalInstructionParser.
        
        Returns:
            Mock SurgicalInstructionParser
        """
        with patch('Backend.Prod.core.orchestrator.SurgicalInstructionParser') as mock_parser_class:
            mock_parser_class.is_surgical_output.return_value = True
            mock_parser_class.parse_instructions.return_value = (
                [Mock(code="def new_method(self):\n    return \"new method\"")],
                None
            )
            return mock_parser_class


class LogCaptureService:
    """Service for capturing and analyzing logs."""
    
    @staticmethod
    def capture_logs(caplog, level: int = logging.INFO) -> List[logging.LogRecord]:
        """
        Capture logs at specified level.
        
        Args:
            caplog: pytest caplog fixture
            level: Logging level
            
        Returns:
            List of log records
        """
        caplog.set_level(level)
        return caplog.records
    
    @staticmethod
    def filter_logs_by_message(records: List[logging.LogRecord], keyword: str) -> List[logging.LogRecord]:
        """
        Filter logs by message keyword.
        
        Args:
            records: List of log records
            keyword: Keyword to search for
            
        Returns:
            Filtered log records
        """
        return [record for record in records if keyword in record.message]
    
    @staticmethod
    def extract_surgical_mode_status(records: List[logging.LogRecord]) -> Dict[str, Any]:
        """
        Extract surgical mode status from logs.
        
        Args:
            records: List of log records
            
        Returns:
            Dictionary with surgical mode information
        """
        surgical_logs = LogCaptureService.filter_logs_by_message(records, "Surgical mode:")
        
        if not surgical_logs:
            return {"found": False, "status": None, "details": {}}
        
        # Parse the log message
        log_message = surgical_logs[0].message
        status = "True" in log_message
        
        # Extract details
        details = {}
        if "execution_mode=" in log_message:
            start = log_message.find("execution_mode=") + len("execution_mode=")
            end = log_message.find(",", start)
            details["execution_mode"] = log_message[start:end] if end != -1 else log_message[start:]
        
        if "step_type=" in log_message:
            start = log_message.find("step_type=") + len("step_type=")
            end = log_message.find(",", start)
            details["step_type"] = log_message[start:end] if end != -1 else log_message[start:]
        
        if "has_python_files=" in log_message:
            start = log_message.find("has_python_files=") + len("has_python_files=")
            end = log_message.find(")", start)
            details["has_python_files"] = log_message[start:end] if end != -1 else log_message[start:]
        
        return {
            "found": True,
            "status": status,
            "details": details,
            "raw_message": log_message
        }


# ============================================================================
# TEST FIXTURES
# ============================================================================

@pytest.fixture
def temp_project_dir():
    """Create a temporary project directory with Python files."""
    temp_dir = tempfile.mkdtemp(prefix="aetherflow_test_")
    project_dir = Path(temp_dir) / "test_project"
    project_dir.mkdir(parents=True)
    
    # Create Python files for testing
    python_file = project_dir / "test_module.py"
    python_file.write_text("""
import os
from typing import Optional

class TestClass:
    def existing_method(self) -> str:
        return "existing"
        
def existing_function() -> int:
    return 42
""")
    
    another_file = project_dir / "another_module.py"
    another_file.write_text("""
from fastapi import APIRouter

router = APIRouter()

@router.get("/test")
async def test_endpoint():
    return {"status": "ok"}
""")
    
    yield project_dir
    
    # Cleanup
    shutil.rmtree(temp_dir)


@pytest.fixture
def test_plan_factory():
    """Provide test plan factory."""
    return TestPlanFactory


@pytest.fixture
def mock_services():
    """Provide mock services."""
    return {
        "agent_router": MockAgentRouterService,
        "surgical_editor": MockSurgicalEditorService,
        "log_capture": LogCaptureService
    }


# ============================================================================
# TEST CLASSES
# ============================================================================

class TestSurgicalModeActivation:
    """Test surgical mode activation based on execution mode."""
    
    @pytest.mark.asyncio
    @pytest.mark.parametrize("execution_mode,expected_surgical_mode", [
        ("FAST", False),
        ("BUILD", True),
        ("DOUBLE-CHECK", True),
    ])
    async def test_surgical_mode_by_execution_mode(
        self,
        temp_project_dir,
        test_plan_factory,
        mock_services,
        execution_mode,
        expected_surgical_mode,
        caplog
    ):
        """
        Test that surgical mode is correctly enabled/disabled based on execution mode.
        
        Args:
            execution_mode: Execution mode to test
            expected_surgical_mode: Expected surgical mode status
        """
        # Setup
        plan = test_plan_factory.create_surgical_mode_test_plan(temp_project_dir)
        mock_router = mock_services["agent_router"].create_successful_router()
        
        with mock_services["surgical_editor"].create_mock_editor() as mock_editor, \
             mock_services["surgical_editor"].create_mock_parser() as mock_parser:
            
            orchestrator = Orchestrator(
                project_root=temp_project_dir,
                execution_mode=execution_mode,
                agent_router=mock_router
            )
            
            # Execute plan
            results = await orchestrator.execute_plan(plan)
            
            # Verify results
            assert results["step_1"].success
            
            # Analyze logs
            records = mock_services["log_capture"].capture_logs(caplog)
            surgical_info = mock_services["log_capture"].extract_surgical_mode_status(records)
            
            assert surgical_info["found"], "No surgical mode log found"
            assert surgical_info["status"] == expected_surgical_mode, \
                f"Surgical mode should be {expected_surgical_mode} in {execution_mode} mode"
            
            # Verify execution mode in logs
            assert surgical_info["details"].get("execution_mode") == execution_mode, \
                f"execution_mode={execution_mode} should be in logs"
            
            # Verify agent router call
            mock_router.execute_step.assert_called_once()
            call_kwargs = mock_router.execute_step.call_args[1]
            assert call_kwargs.get('surgical_mode') == expected_surgical_mode, \
                f"surgical_mode should be {expected_surgical_mode} when passed to agent_router"
            
            # Verify AST parsing based on surgical mode
            if expected_surgical_mode:
                mock_editor.return_value.prepare.assert_called()
                mock_editor.return_value.get_ast_context.assert_called()
            else:
                mock_editor.return_value.prepare.assert_not_called()
    
    @pytest.mark.asyncio
    async def test_same_plan_different_modes_comparison(
        self,
        temp_project_dir,
        test_plan_factory,
        mock_services,
        caplog
    ):
        """Execute the same plan in FAST and BUILD modes and compare surgical mode status."""
        # Create plan
        plan = test_plan_factory.create_surgical_mode_test_plan(temp_project_dir)
        
        # Track results
        surgical_mode_results = {}
        
        # Test FAST mode
        mock_router_fast = mock_services["agent_router"].create_successful_router()
        
        with mock_services["surgical_editor"].create_mock_editor() as mock_editor_fast:
            orchestrator_fast = Orchestrator(
                project_root=temp_project_dir,
                execution_mode="FAST",
                agent_router=mock_router_fast
            )
            
            records = mock_services["log_capture"].capture_logs(caplog)
            await orchestrator_fast.execute_plan(plan)
            
            surgical_info_fast = mock_services["log_capture"].extract_surgical_mode_status(records)
            surgical_mode_results["FAST"] = surgical_info_fast.get("status", None)
        
        # Test BUILD mode
        mock_router_build = mock_services["agent_router"].create_successful_router()
        
        with mock_services["surgical_editor"].create_mock_editor() as mock_editor_build:
            orchestrator_build = Orchestrator(
                project_root=temp_project_dir,
                execution_mode="BUILD",
                agent_router=mock_router_build
            )
            
            records = mock_services["log_capture"].capture_logs(caplog)
            await orchestrator_build.execute_plan(plan)
            
            surgical_info_build = mock_services["log_capture"].extract_surgical_mode_status(records)
            surgical_mode_results["BUILD"] = surgical_info_build.get("status", None)
        
        # Verify the difference
        assert surgical_mode_results["FAST"] == False, "Surgical mode should be False in FAST mode"
        assert surgical_mode_results["BUILD"] == True, "Surgical mode should be True in BUILD mode"
        
        print(f"\nComparison test results:")
        print(f"  FAST mode: surgical_mode={surgical_mode_results['FAST']}")
        print(f"  BUILD mode: surgical_mode={surgical_mode_results['BUILD']}")


class TestSurgicalModeEdgeCases:
    """Test edge cases for surgical mode activation."""
    
    @pytest.mark.asyncio
    async def test_surgical_mode_with_new_files_only(
        self,
        temp_project_dir,
        test_plan_factory,
        mock_services,
        caplog
    ):
        """Test that surgical mode is disabled when only new files are involved."""
        # Create a plan with only new files
        new_file_path = temp_project_dir / "new_file.py"
        plan = test_plan_factory.create_surgical_mode_test_plan(
            temp_project_dir,
            existing_files=[str(new_file_path)]
        )
        
        mock_router = mock_services["agent_router"].create_successful