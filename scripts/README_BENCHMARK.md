# Scripts de Benchmarking AETHERFLOW

**Date** : 26 janvier 2025

---

## üìä Scripts Disponibles

### 1. `benchmark.py` - Benchmark de Base

Script de benchmark simple avec m√©triques de base.

**Usage** :
```bash
python scripts/benchmark.py --plan path/to/plan.json --output output/dir
```

**G√©n√®re** :
- `benchmark_report.md` : Rapport markdown
- `benchmark_data.json` : Donn√©es JSON

---

### 2. `benchmark_comprehensive.py` - Benchmark Complet ‚≠ê NOUVEAU

Script de benchmark am√©lior√© avec :
- ‚úÖ M√©triques de latence (TTFT, TTR, queue latency, network overhead)
- ‚úÖ Comparaison par provider
- ‚úÖ Comparaison par type de t√¢che
- ‚úÖ M√©triques de cache (si disponibles)
- ‚úÖ Tableaux r√©capitulatifs dans le terminal

**Usage** :
```bash
python scripts/benchmark_comprehensive.py --plan path/to/plan.json --output output/dir
```

**G√©n√®re** :
- `benchmark_report.md` : Rapport markdown complet avec analyses
- `benchmark_data.json` : Donn√©es JSON d√©taill√©es

**Exemple** :
```bash
python scripts/benchmark_comprehensive.py \
  --plan Backend/Notebooks/benchmark_tasks/task_parallelization.json \
  --output output/benchmark_comprehensive
```

---

### 3. `run_benchmark_suite.py` - Suite de Benchmarks ‚≠ê NOUVEAU

Script pour ex√©cuter plusieurs benchmarks et comparer les r√©sultats.

**Usage** :
```bash
python scripts/run_benchmark_suite.py \
  --plans plan1.json plan2.json plan3.json \
  --output output/benchmark_suite
```

**G√©n√®re** :
- `comparison_report.md` : Rapport de comparaison
- `comparison_data.json` : Donn√©es de comparaison JSON
- Un r√©pertoire par plan avec ses r√©sultats individuels

**Exemple** :
```bash
python scripts/run_benchmark_suite.py \
  --plans \
    Backend/Notebooks/benchmark_tasks/task_parallelization.json \
    Backend/Notebooks/benchmark_tasks/task_test_parallelization.json \
  --output output/benchmark_suite
```

---

## üìà M√©triques Collect√©es

### M√©triques de Base
- ‚úÖ Temps d'ex√©cution total
- ‚úÖ Co√ªt total
- ‚úÖ Tokens utilis√©s (input/output)
- ‚úÖ Taux de succ√®s
- ‚úÖ D√©tails par √©tape

### M√©triques de Latence (Nouvelles)
- ‚è≥ **TTFT** (Time To First Token) - Temps avant premier token
- ‚è≥ **TTR** (Time To Response) - Temps total de r√©ponse
- ‚è≥ **Queue Latency** - Temps d'attente en file
- ‚è≥ **Network Overhead** - Overhead r√©seau (DNS + TCP + TLS)

*Note* : Ces m√©triques seront disponibles une fois les optimisations de latence impl√©ment√©es.

### M√©triques de Cache (Futures)
- ‚è≥ **Cache Hit Rate** - Taux de hits du cache prompt
- ‚è≥ **Cache Read Cost** - Co√ªt des lectures cache

### M√©triques de Qualit√© (Futures)
- ‚è≥ **Code Quality Score** - Score qualit√© code g√©n√©r√©
- ‚è≥ **First Try Success Rate** - % code fonctionnel du premier coup

---

## üîç Comparaisons Disponibles

### Par Provider
Le script `benchmark_comprehensive.py` compare automatiquement :
- Temps moyen par provider
- Co√ªt moyen par provider
- Tokens moyens par provider
- Taux de succ√®s par provider
- TTFT moyen par provider (si disponible)

### Par Type de T√¢che
Comparaison automatique pour :
- `code_generation`
- `refactoring`
- `analysis`

### Avant/Apr√®s Optimisations
Utilisez `run_benchmark_suite.py` pour comparer :
- Baseline vs Optimis√©
- Avant/apr√®s parall√©lisation
- Avant/apr√®s prompt caching
- Avant/apr√®s SLM locaux

---

## üìã Exemples d'Utilisation

### Benchmark Simple
```bash
python scripts/benchmark.py \
  --plan Backend/Notebooks/benchmark_tasks/task_test_parallelization.json \
  --output output/benchmark_simple
```

### Benchmark Complet avec Analyses
```bash
python scripts/benchmark_comprehensive.py \
  --plan Backend/Notebooks/benchmark_tasks/task_test_parallelization.json \
  --output output/benchmark_comprehensive
```

### Suite de Benchmarks
```bash
python scripts/run_benchmark_suite.py \
  --plans \
    Backend/Notebooks/benchmark_tasks/task_parallelization.json \
    Backend/Notebooks/benchmark_tasks/task_test_parallelization.json \
    Backend/Notebooks/benchmark_tasks/task_rag_pageindex.json \
  --output output/benchmark_suite
```

### Comparaison Baseline vs Optimis√©
```bash
# Ex√©cuter baseline
python scripts/run_benchmark_suite.py \
  --plans plan1.json plan2.json \
  --output output/baseline \
  --baseline-label baseline

# Ex√©cuter optimis√© (apr√®s optimisations)
python scripts/run_benchmark_suite.py \
  --plans plan1.json plan2.json \
  --output output/optimized \
  --optimized-label optimized

# Comparer manuellement les rapports g√©n√©r√©s
```

---

## üìä Format des Rapports

### Rapport Markdown
Chaque script g√©n√®re un rapport markdown avec :
- M√©triques globales
- Performance (temps)
- Co√ªts
- Tokens
- Analyse par provider
- Analyse par type
- D√©tails par √©tape
- Questions pour Claude Code

### Donn√©es JSON
Les donn√©es JSON contiennent toutes les m√©triques d√©taill√©es pour :
- Analyse programmatique
- G√©n√©ration de graphiques
- Comparaisons automatiques

---

## üöÄ Prochaines √âtapes

### Court Terme
1. ‚úÖ Scripts de base cr√©√©s
2. ‚è≥ Tester avec plans existants
3. ‚è≥ Valider les m√©triques collect√©es

### Moyen Terme
1. ‚è≥ Ajouter g√©n√©ration de graphiques (matplotlib/plotly)
2. ‚è≥ Ajouter comparaisons automatiques avant/apr√®s
3. ‚è≥ Int√©grer m√©triques de latence r√©elles (TTFT, TTR)

### Long Terme
1. ‚è≥ Scripts sp√©cifiques par optimisation :
   - `benchmark_prompt_cache.py`
   - `benchmark_slm_local.py`
   - `benchmark_speculative.py`
   - `benchmark_semantic_cache.py`

---

## üìö R√©f√©rences

- `/docs/guides/STRATEGIE_BENCHMARK_LATENCE.md` : Strat√©gie compl√®te de benchmark
- `/docs/guides/PLAN_GENERAL_ROADMAP.md` : √âtape 6 - Am√©liorer scripts benchmarking
- `/docs/guides/PRD AETHERFLOW.md` : Section m√©triques et surveillance

---

**Derni√®re mise √† jour** : 26 janvier 2025
