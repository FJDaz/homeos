#!/usr/bin/env python3
"""Benchmark m√©ta : Tester AETHERFLOW 0.1 pour construire AETHERFLOW 0.2 (Codestral).

Ce script mesure les performances de l'association Claude Code + DeepSeek
pour impl√©menter CodestralClient (Phase 2.1).
"""
import asyncio
import json
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, Any
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich import box

# Ajouter le r√©pertoire parent au path
sys.path.insert(0, str(Path(__file__).parent.parent))

from Backend.Prod.orchestrator import Orchestrator, ExecutionError
from Backend.Prod.plan_reader import PlanReader, PlanValidationError

console = Console()


async def benchmark_codestral_implementation(plan_path: Path, output_dir: Path) -> Dict[str, Any]:
    """
    Ex√©cute le plan d'impl√©mentation de Codestral et collecte les m√©triques.
    
    Args:
        plan_path: Chemin vers le plan JSON
        output_dir: R√©pertoire de sortie
        
    Returns:
        Dictionnaire avec toutes les m√©triques
    """
    console.print(Panel.fit(
        "[bold cyan]AETHERFLOW 0.1 ‚Üí AETHERFLOW 0.2[/]\n"
        "[dim]Benchmark m√©ta : Claude Code + DeepSeek construisent Codestral[/]",
        border_style="cyan"
    ))
    
    console.print(f"\n[cyan]üìä Ex√©cution du plan: {plan_path.name}[/]")
    
    orchestrator = Orchestrator()
    
    try:
        # Ex√©cuter le plan
        start_time = datetime.now()
        result = await orchestrator.execute_plan(
            plan_path=plan_path,
            output_dir=output_dir
        )
        end_time = datetime.now()
        
        metrics = result["metrics"]
        plan = result["plan"]
        
        # Calculer le temps total (incluant overhead)
        total_wall_time = (end_time - start_time).total_seconds() * 1000
        
        # Collecter les m√©triques d√©taill√©es
        benchmark_data = {
            "benchmark_type": "meta_benchmark",
            "benchmark_description": "AETHERFLOW 0.1 (Claude Code + DeepSeek) construit AETHERFLOW 0.2 (Codestral)",
            "task_id": plan.task_id,
            "task_description": plan.description,
            "execution_date": datetime.now().isoformat(),
            "plan_file": str(plan_path),
            "output_dir": str(output_dir),
            
            # M√©triques globales
            "success": result["success"],
            "total_steps": metrics.total_steps,
            "successful_steps": metrics.successful_steps,
            "failed_steps": metrics.failed_steps,
            "success_rate": metrics.success_rate,
            
            # Performance
            "total_execution_time_ms": metrics.total_execution_time_ms,
            "total_wall_time_ms": total_wall_time,
            "overhead_ms": total_wall_time - metrics.total_execution_time_ms,
            "average_step_time_ms": metrics.average_step_time_ms,
            
            # Co√ªts
            "total_cost_usd": metrics.total_cost_usd,
            "cost_per_step": metrics.total_cost_usd / metrics.total_steps if metrics.total_steps > 0 else 0,
            
            # Tokens
            "total_tokens": metrics.total_tokens_used,
            "total_input_tokens": metrics.total_input_tokens,
            "total_output_tokens": metrics.total_output_tokens,
            "tokens_per_step": metrics.total_tokens_used / metrics.total_steps if metrics.total_steps > 0 else 0,
            
            # D√©tails par √©tape
            "step_details": [
                {
                    "step_id": step_m["step_id"],
                    "description": step_m["step_description"],
                    "type": step_m["step_type"],
                    "complexity": next(
                        (s.complexity for s in plan.steps if s.id == step_m["step_id"]),
                        0.0
                    ),
                    "success": step_m["success"],
                    "time_ms": step_m["execution_time_ms"],
                    "tokens": step_m["tokens_used"],
                    "cost": step_m["cost_usd"],
                    "error": step_m.get("error")
                }
                for step_m in metrics.step_metrics
            ],
            
            # Fichiers g√©n√©r√©s
            "generated_files": [
                str(output_dir / "step_outputs" / f"{step.id}.txt")
                for step in plan.steps
                if (output_dir / "step_outputs" / f"{step.id}.txt").exists()
            ],
            
            # M√©triques sp√©cifiques au benchmark m√©ta
            "meta_metrics": {
                "self_hosting_success": result["success"],
                "code_quality": "to_be_evaluated",  # √Ä √©valuer manuellement
                "implementation_completeness": metrics.success_rate,
                "cost_efficiency": metrics.total_cost_usd,
                "time_efficiency_ms": metrics.total_execution_time_ms
            }
        }
        
        return benchmark_data
        
    except ExecutionError as e:
        console.print(f"[red]‚úó Erreur d'ex√©cution: {e}[/]")
        return {
            "benchmark_type": "meta_benchmark",
            "task_id": plan_path.stem,
            "success": False,
            "error": str(e),
            "execution_date": datetime.now().isoformat()
        }
    except Exception as e:
        console.print(f"[red]‚úó Erreur inattendue: {e}[/]")
        return {
            "benchmark_type": "meta_benchmark",
            "task_id": plan_path.stem,
            "success": False,
            "error": str(e),
            "execution_date": datetime.now().isoformat()
        }
    finally:
        await orchestrator.close()


def generate_markdown_report(benchmark_data: Dict[str, Any], output_file: Path) -> None:
    """
    G√©n√®re un rapport markdown pour analyser le benchmark m√©ta.
    
    Args:
        benchmark_data: Donn√©es de benchmark
        output_file: Fichier de sortie markdown
    """
    md_lines = []
    
    md_lines.append("# Rapport de Benchmark M√©ta - AETHERFLOW 0.1 ‚Üí 0.2\n")
    md_lines.append(f"**Date** : {benchmark_data.get('execution_date', 'N/A')}\n")
    md_lines.append(f"**Type** : Benchmark m√©ta (self-hosting)\n")
    md_lines.append(f"**Description** : {benchmark_data.get('benchmark_description', 'N/A')}\n")
    md_lines.append(f"**Task ID** : `{benchmark_data.get('task_id', 'N/A')}`\n")
    md_lines.append("")
    
    # Statut
    success = benchmark_data.get("success", False)
    status_emoji = "‚úÖ" if success else "‚ùå"
    md_lines.append(f"## Statut d'Ex√©cution\n\n{status_emoji} **{'Succ√®s' if success else '√âchec'}**\n")
    
    if not success:
        md_lines.append(f"**Erreur** : {benchmark_data.get('error', 'Unknown')}\n")
        md_lines.append("")
        output_file.write_text("\n".join(md_lines), encoding="utf-8")
        return
    
    # M√©triques globales
    md_lines.append("## M√©triques Globales\n")
    md_lines.append("| M√©trique | Valeur |")
    md_lines.append("|----------|--------|")
    md_lines.append(f"| Total √©tapes | {benchmark_data.get('total_steps', 0)} |")
    md_lines.append(f"| √âtapes r√©ussies | {benchmark_data.get('successful_steps', 0)} |")
    md_lines.append(f"| √âtapes √©chou√©es | {benchmark_data.get('failed_steps', 0)} |")
    md_lines.append(f"| Taux de r√©ussite | {benchmark_data.get('success_rate', 0):.1%} |")
    md_lines.append("")
    
    # Performance
    md_lines.append("## Performance\n")
    md_lines.append("| M√©trique | Valeur |")
    md_lines.append("|----------|--------|")
    md_lines.append(f"| Temps d'ex√©cution total | {benchmark_data.get('total_execution_time_ms', 0) / 1000:.2f}s |")
    md_lines.append(f"| Temps wall-clock total | {benchmark_data.get('total_wall_time_ms', 0) / 1000:.2f}s |")
    md_lines.append(f"| Overhead | {benchmark_data.get('overhead_ms', 0) / 1000:.2f}s |")
    md_lines.append(f"| Temps moyen par √©tape | {benchmark_data.get('average_step_time_ms', 0):.0f}ms |")
    md_lines.append("")
    
    # Co√ªts
    md_lines.append("## Co√ªts\n")
    md_lines.append("| M√©trique | Valeur |")
    md_lines.append("|----------|--------|")
    md_lines.append(f"| Co√ªt total | ${benchmark_data.get('total_cost_usd', 0):.4f} |")
    md_lines.append(f"| Co√ªt par √©tape | ${benchmark_data.get('cost_per_step', 0):.4f} |")
    md_lines.append("")
    
    # Tokens
    md_lines.append("## Utilisation de Tokens\n")
    md_lines.append("| M√©trique | Valeur |")
    md_lines.append("|----------|--------|")
    md_lines.append(f"| Tokens total | {benchmark_data.get('total_tokens', 0):,} |")
    md_lines.append(f"| Tokens input | {benchmark_data.get('total_input_tokens', 0):,} |")
    md_lines.append(f"| Tokens output | {benchmark_data.get('total_output_tokens', 0):,} |")
    md_lines.append(f"| Tokens par √©tape | {benchmark_data.get('tokens_per_step', 0):.0f} |")
    md_lines.append("")
    
    # D√©tails par √©tape
    md_lines.append("## D√©tails par √âtape\n")
    for step in benchmark_data.get("step_details", []):
        status = "‚úÖ" if step.get("success") else "‚ùå"
        complexity = step.get("complexity", 0.0)
        md_lines.append(f"### {status} {step.get('step_id', 'N/A')} (complexity: {complexity:.2f})\n")
        md_lines.append(f"- **Description** : {step.get('description', 'N/A')}")
        md_lines.append(f"- **Type** : {step.get('type', 'N/A')}")
        md_lines.append(f"- **Temps** : {step.get('time_ms', 0):.0f}ms")
        md_lines.append(f"- **Tokens** : {step.get('tokens', 0):,}")
        md_lines.append(f"- **Co√ªt** : ${step.get('cost', 0):.4f}")
        if step.get("error"):
            md_lines.append(f"- **Erreur** : {step['error']}")
        md_lines.append("")
    
    # M√©triques m√©ta
    meta_metrics = benchmark_data.get("meta_metrics", {})
    md_lines.append("## M√©triques M√©ta (Self-Hosting)\n")
    md_lines.append("| M√©trique | Valeur |")
    md_lines.append("|----------|--------|")
    md_lines.append(f"| Succ√®s self-hosting | {'‚úÖ' if meta_metrics.get('self_hosting_success') else '‚ùå'} |")
    md_lines.append(f"| Compl√©tude impl√©mentation | {meta_metrics.get('implementation_completeness', 0):.1%} |")
    md_lines.append(f"| Efficacit√© co√ªt | ${meta_metrics.get('cost_efficiency', 0):.4f} |")
    md_lines.append(f"| Efficacit√© temps | {meta_metrics.get('time_efficiency_ms', 0) / 1000:.2f}s |")
    md_lines.append("")
    
    # Fichiers g√©n√©r√©s
    generated_files = benchmark_data.get("generated_files", [])
    if generated_files:
        md_lines.append("## Fichiers G√©n√©r√©s\n")
        for file_path in generated_files:
            md_lines.append(f"- `{file_path}`")
        md_lines.append("")
    
    # Analyse pour Claude Code
    md_lines.append("## Analyse pour Claude Code\n")
    md_lines.append("Claude Code, analyse ce benchmark m√©ta et r√©ponds :\n")
    md_lines.append("1. **Self-Hosting** : AETHERFLOW a-t-il r√©ussi √† se construire lui-m√™me ?")
    md_lines.append("2. **Qualit√© du code g√©n√©r√©** : Le code de CodestralClient est-il de bonne qualit√© ?")
    md_lines.append("3. **Performance** : Les temps d'ex√©cution sont-ils acceptables ?")
    md_lines.append("4. **Co√ªts** : Les co√ªts sont-ils raisonnables pour cette t√¢che complexe ?")
    md_lines.append("5. **Efficacit√©** : AETHERFLOW est-il efficace pour construire ses propres composants ?")
    md_lines.append("6. **Am√©liorations** : Quelles am√©liorations sugg√®res-tu pour AETHERFLOW ?")
    md_lines.append("")
    
    # √âcrire le fichier
    output_file.parent.mkdir(parents=True, exist_ok=True)
    output_file.write_text("\n".join(md_lines), encoding="utf-8")
    console.print(f"[green]‚úì Rapport g√©n√©r√©: {output_file}[/]")


def print_summary_table(benchmark_data: Dict[str, Any]) -> None:
    """Affiche un tableau r√©capitulatif dans la console."""
    table = Table(title="R√©sum√© du Benchmark M√©ta", box=box.ROUNDED)
    table.add_column("M√©trique", style="cyan")
    table.add_column("Valeur", style="green")
    
    success = benchmark_data.get("success", False)
    table.add_row("Statut", "‚úÖ Succ√®s" if success else "‚ùå √âchec")
    table.add_row("√âtapes r√©ussies", f"{benchmark_data.get('successful_steps', 0)}/{benchmark_data.get('total_steps', 0)}")
    table.add_row("Temps total", f"{benchmark_data.get('total_execution_time_ms', 0) / 1000:.2f}s")
    table.add_row("Co√ªt total", f"${benchmark_data.get('total_cost_usd', 0):.4f}")
    table.add_row("Tokens total", f"{benchmark_data.get('total_tokens', 0):,}")
    
    console.print("\n")
    console.print(table)


async def main():
    """Point d'entr√©e principal."""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Benchmark m√©ta : AETHERFLOW 0.1 construit AETHERFLOW 0.2 (Codestral)"
    )
    parser.add_argument(
        "--plan",
        type=Path,
        default=Path("Backend/Notebooks/benchmark_tasks/task_10_phase2_codestral_implementation.json"),
        help="Chemin vers le plan JSON (d√©faut: task_10_phase2_codestral_implementation.json)"
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=None,
        help="R√©pertoire de sortie (d√©faut: output/benchmark_meta_codestral)"
    )
    parser.add_argument(
        "--report",
        type=Path,
        default=None,
        help="Fichier de rapport markdown (d√©faut: output_dir/benchmark_report.md)"
    )
    
    args = parser.parse_args()
    
    # D√©terminer les chemins
    plan_path = args.plan
    if not plan_path.exists():
        console.print(f"[red]Erreur: Plan non trouv√©: {plan_path}[/]")
        sys.exit(1)
    
    output_dir = args.output or Path("output") / "benchmark_meta_codestral"
    report_file = args.report or output_dir / "benchmark_report.md"
    
    # Ex√©cuter le benchmark
    benchmark_data = await benchmark_codestral_implementation(plan_path, output_dir)
    
    # Afficher le r√©sum√©
    if benchmark_data.get("success"):
        print_summary_table(benchmark_data)
    
    # G√©n√©rer le rapport
    if benchmark_data.get("success"):
        generate_markdown_report(benchmark_data, report_file)
        
        # Sauvegarder aussi en JSON
        json_file = output_dir / "benchmark_data.json"
        json_file.parent.mkdir(parents=True, exist_ok=True)
        with open(json_file, "w", encoding="utf-8") as f:
            json.dump(benchmark_data, f, indent=2, ensure_ascii=False)
        
        console.print(f"\n[green]‚úì Benchmark termin√©[/]")
        console.print(f"  Rapport: {report_file}")
        console.print(f"  Donn√©es: {json_file}")
    else:
        console.print(f"\n[red]‚úó Benchmark √©chou√©[/]")
        console.print(f"  Erreur: {benchmark_data.get('error', 'Unknown')}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
