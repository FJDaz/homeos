# Tests √âtape 9 - Guide de Lancement

**Date** : 26 janvier 2025  
**Statut** : ‚úÖ **Pr√™t pour tests**  
**R√©f√©rence** : Voir `ETAPE_9_REDUCTION_LATENCE.md` pour vue d'ensemble

---

## ‚úÖ Impl√©mentations Compl√©t√©es

1. ‚úÖ **Speculative Decoding** : Module cr√©√© et int√©gr√©
2. ‚úÖ **Cache S√©mantique** : Module cr√©√© et int√©gr√©  
3. ‚úÖ **Connection Pooling** : Module cr√©√© (httpx g√®re d√©j√† automatiquement)

---

## üß™ Tests Disponibles

### **1. Test Speculative Decoding**

**Script** : `scripts/benchmark_speculative.py`

**Usage** :
```bash
python scripts/benchmark_speculative.py
```

**Ce qu'il teste** :
- Comparaison speculative vs normal execution
- Mesure accept rate, speedup factor
- Temps d'ex√©cution comparatif
- Co√ªt comparatif

**R√©sultats** : Sauvegard√©s dans `output/benchmark_speculative.json`

---

### **2. Test Cache S√©mantique**

**Test manuel** :
```python
from Backend.Prod.cache import SemanticCache

cache = SemanticCache()

# Premier appel (cache MISS)
result1 = cache.get("Generate a REST API")
# None (pas de cache)

# Mettre en cache
cache.put("Generate a REST API", "def api(): ...")

# Deuxi√®me appel similaire (cache HIT)
result2 = cache.get("Create a REST API endpoint")
# Retourne r√©ponse cach√©e si similarit√© > 0.85

# Stats
stats = cache.get_stats()
print(f"Hit rate: {stats.cache_hit_rate:.1f}%")
```

---

### **3. Test Int√©gration Compl√®te**

**Via Orchestrator** :
```python
from Backend.Prod.orchestrator import Orchestrator
from Backend.Prod.models.plan_reader import PlanReader

orchestrator = Orchestrator()
plan = PlanReader().read("path/to/plan.json")

# Ex√©cuter plan (utilise automatiquement toutes les optimisations)
result = await orchestrator.execute_plan(plan)

# V√©rifier m√©triques
print(f"Speculative enabled: {result.get('speculative_enabled')}")
print(f"Cache hits: {result.get('cache_hits')}")
```

---

## üìä M√©triques √† V√©rifier

### **Speculative Decoding** :
- `speculative_accept_rate` : Cible >70%
- `speculative_speedup_factor` : Cible >1.5x
- Temps r√©duit : -30-50% TTFT

### **Cache S√©mantique** :
- `cache_hit_rate` : Cible >40%
- Tokens √©conomis√©s
- Co√ªt √©conomis√©

### **Connection Pooling** :
- `connection_reuse_rate` : Cible >80%
- Overhead r√©seau r√©duit : ~350ms par requ√™te r√©utilis√©e

---

## ‚ö†Ô∏è Notes Importantes

1. **Premier lancement** : Le mod√®le d'embedding (`all-MiniLM-L6-v2`) se t√©l√©charge la premi√®re fois (~80MB)
   - Temps de chargement : ~10-30 secondes
   - Ensuite, chargement instantan√© depuis cache

2. **NumPy** : N√©cessite `numpy<2` (d√©j√† install√©)

3. **D√©pendances** :
   ```bash
   pip install sentence-transformers "numpy<2"
   ```

---

## üöÄ Lancement Rapide

```bash
# 1. V√©rifier d√©pendances
pip install sentence-transformers "numpy<2"

# 2. Lancer benchmark speculative
python scripts/benchmark_speculative.py

# 3. V√©rifier r√©sultats
cat output/benchmark_speculative.json
```

---

## üìù R√©sultats Attendus

**Speculative Decoding** :
- Speedup : 1.5x - 2x pour t√¢ches longues
- Accept rate : 60-80% selon t√¢che
- Latence r√©duite : -30-50%

**Cache S√©mantique** :
- Hit rate : 40-60% pour workflows r√©p√©titifs
- Latence cache hit : <10ms vs 2-5s API
- Co√ªt √©conomis√© : 100% sur cache hits

---

## üîó Documentation

- Speculative Decoding : `/docs/guides/SPECULATIVE_DECODING.md`
- Cache S√©mantique : `/docs/guides/CACHE_SEMANTIQUE.md`
- Connection Pooling : `/docs/guides/WEBSOCKETS_CONNECTION_POOL.md`
- Rapport complet : `/docs/guides/RAPPORT_ETAPE_9_COMPLET.md`
