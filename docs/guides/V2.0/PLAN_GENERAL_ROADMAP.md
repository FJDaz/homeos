# Plan G√©n√©ral AETHERFLOW - Roadmap Compl√®te

**Derni√®re mise √† jour** : 26 janvier 2025  
**Version** : Phase 3 en cours (√âtape 9 termin√©e, √âtape 10 termin√©e)  
**R√©f√©rence √âtape 9** : Voir `ETAPE_9_REDUCTION_LATENCE.md` pour documentation compl√®te  
**R√©f√©rence √âtape 10** : Voir `OPTIMISATIONS_ZERO_BUDGET.md` pour documentation compl√®te

---

## üìä R√©capitulatif Rapide

### ‚úÖ Phase 1 & 2 : TERMIN√âES
- ‚úÖ **√âtape 1** : AgentRouter int√©gr√© dans Orchestrator
- ‚úÖ **√âtape 2** : GeminiClient impl√©ment√©
- ‚úÖ **√âtape 3** : GroqClient impl√©ment√©
- ‚úÖ **√âtape 4** : Routage intelligent impl√©ment√©
- ‚úÖ **√âtape 5.5** : Monitoring temps r√©el impl√©ment√©
- ‚úÖ **√âtape 7** : Parall√©lisation impl√©ment√©e et test√©e ‚úÖ

### ‚è≥ Phase 2 (suite) : EN COURS
- ‚è≥ **√âtape 6** : Am√©liorer scripts benchmarking (priorit√© moyenne)
  - Ajouter m√©triques de latence (TTFT, TTR, cache hit rate)
  - Comparaisons avant/apr√®s optimisations

### ‚úÖ Phase 3 : EN COURS / TERMIN√âES
- ‚úÖ **√âtape 8** : RAG PageIndex pour contexte projet ‚úÖ TERMIN√â
- ‚úÖ **√âtape 9** : R√©duction latence API ‚úÖ TERMIN√â
  - ‚úÖ Prompt caching
  - ‚ùå SLM locaux (ANNUL√â - contrainte technique i7 4 c≈ìurs, utiliser Groq √† la place)
  - ‚úÖ Speculative decoding
  - ‚úÖ Cache s√©mantique
  - ‚úÖ Connection pooling
- ‚úÖ **√âtape 10** : Optimisations "Z√©ro Budget" ‚úÖ TERMIN√â
  - ‚úÖ Streaming pipelining
  - ‚úÖ Modes d'ex√©cution (FAST/BUILD/DOUBLE-CHECK)
  - ‚úÖ Parall√©lisation massive am√©lior√©e
  - ‚úÖ Prompt stripping
  - ‚úÖ Output constraints

---

## √âtat Actuel vs PRD

### Ce qui est fait (Phase 1 & 2 Option B)
- ‚úÖ CLI simplifi√©e (`cli_generate.py`) avec AgentRouter
- ‚úÖ BaseLLMClient interface commune
- ‚úÖ DeepSeekClient impl√©ment√© et fonctionnel
- ‚úÖ CodestralClient impl√©ment√© et fonctionnel
- ‚úÖ GeminiClient impl√©ment√© et fonctionnel
- ‚úÖ GroqClient impl√©ment√© et fonctionnel
- ‚úÖ AgentRouter avec support multi-providers (DeepSeek + Codestral + Gemini + Groq)
- ‚úÖ **Orchestrator utilise AgentRouter** (ligne 38) ‚úÖ **FAIT**
- ‚úÖ **Routage intelligent impl√©ment√©** (`select_provider_for_step()` avec logique compl√®te) ‚úÖ **FAIT**
- ‚úÖ Configuration compl√®te (settings.py, .env.example)
- ‚úÖ Check de balance impl√©ment√©
- ‚úÖ Scripts de benchmarking de base
- ‚úÖ Co√ªts Groq configur√©s dans settings.py

### Ce qui manque vs PRD
- ‚ùå Pas de parall√©lisation des t√¢ches ind√©pendantes (commentaire ligne 97 orchestrator.py) - **√Ä FAIRE MAINTENANT**
- ‚ùå Scripts benchmarking √† am√©liorer (priorit√© moyenne)
- ‚ùå Pas de RAG (ChromaDB) pour contexte projet (Phase 3, peut √™tre diff√©r√©)

### D√©cisions r√©centes
- ‚ùå **Tracking temps Claude Code** : Annul√© (m√©trique arbitraire, pas de valeur op√©rationnelle r√©elle)
  - Voir `/docs/guides/ANALYSE_METRIQUE_TEMPS_CLAUDE.md` pour l'analyse compl√®te

---

## Architecture Actuelle (Phase 2 Compl√©t√©e)

```
Claude Code (Cursor) - Moi
    ‚Üì G√©n√®re plan.json OU appelle CLI directement
    ‚Üì √âconomie : -83% tokens, -60% utilisations fast premium
    ‚Üì
AETHERFLOW Orchestrator ‚úÖ
    ‚Üì Utilise AgentRouter ‚úÖ
    ‚Üì Monitoring temps r√©el ‚úÖ NOUVEAU
    ‚Üì
AgentRouter (Routage Intelligent) ‚úÖ IMPL√âMENT√â
    ‚Üì S√©lectionne provider selon type/complexity/tokens ‚úÖ
    ‚Üì Routage automatique : Gemini (analysis), Codestral (refactoring), 
    ‚Üì                      DeepSeek (code_generation), Groq (prototyping)
    ‚Üì
Providers: DeepSeek ‚úÖ | Codestral ‚úÖ | Gemini ‚úÖ | Groq ‚úÖ
    ‚Üì Co√ªt moyen : ~$0.0008 par t√¢che (routage intelligent maximise Gemini gratuit)
    ‚Üì
Code g√©n√©r√© + M√©triques ‚úÖ
    ‚Üì Affichage temps r√©el ‚úÖ NOUVEAU (progression, provider, temps, co√ªts)
    ‚Üì
Benchmarking automatique ‚úÖ (sans temps Claude Code)
    ‚Üì Mesure temps Claude Code gagn√© ‚è≥ (code g√©n√©r√©, int√©gration en attente)
    ‚Üì
Claude Code (Cursor) - Moi
    ‚Üì V√©rifie et pr√©sente les r√©sultats
    ‚Üì
Rapport d'√©valuation ‚úÖ
```

**Statut** : Architecture Phase 2 op√©rationnelle. Routage intelligent fonctionnel avec 4 providers.

**Clarifications importantes** :
- ‚úÖ **Claude Code (dans Cursor)** = Moi, g√©n√®re plans et v√©rifie r√©sultats
- ‚ùå **Claude API (Anthropic)** = NON utilis√© dans AETHERFLOW
- ‚úÖ **AETHERFLOW** = Utilise DeepSeek/Gemini/Codestral/Groq (ind√©pendant de Claude API)
- ‚úÖ **√âconomies** : -83% tokens Claude Code, -60% utilisations fast premium, -50% temps total

---

## Principe de Meta-Benchmark : AETHERFLOW se construit lui-m√™me

**Approche de benchmarking** : Chaque nouvelle √©tape est construite **VIA AETHERFLOW**, ce qui sert de benchmark.

**Workflow pour chaque √©tape** :
1. **Claude Code (Moi)** g√©n√®re un `plan.json` d√©crivant comment construire l'√©tape X
   - Utilise 1 requ√™te fast premium (~1,500 tokens)
   - √âconomie : vs g√©n√©ration directe de code (~13,800 tokens)
2. **AETHERFLOW** ex√©cute ce plan pour g√©n√©rer le code de l'√©tape X
   - Routage intelligent s√©lectionne automatiquement le meilleur provider
   - Co√ªt moyen : ~$0.0008 par t√¢che (Gemini gratuit pour analyses)
3. **Mesure automatique** : temps, co√ªt, qualit√©, tokens utilis√©s
4. **Claude Code (Moi)** v√©rifie les r√©sultats
   - Utilise 1 requ√™te fast premium (~800 tokens)
   - Total : 2 requ√™tes fast premium par t√¢che (vs 5 sans AETHERFLOW)
5. **Calcul temps Claude Code gagn√©** : temps estim√© manuel vs temps r√©el AETHERFLOW

**Exemple** : Pour impl√©menter GeminiClient (√âtape 2) :
- Claude Code cr√©e `task_gemini_client.json` avec les √©tapes de g√©n√©ration (1 requ√™te, ~1,500 tokens)
- AETHERFLOW ex√©cute ce plan ‚Üí g√©n√®re `gemini_client.py` (Gemini pour analysis, DeepSeek pour code)
- Claude Code v√©rifie les r√©sultats (1 requ√™te, ~800 tokens)
- Benchmark = mesurer les performances de cette g√©n√©ration
- R√©sultat : "AETHERFLOW a g√©n√©r√© GeminiClient en 2min, √©conomisant 28min vs impl√©mentation manuelle"
- **√âconomie tokens** : 2,300 tokens vs 13,800 tokens (-83%)
- **√âconomie requ√™tes** : 2 requ√™tes vs 5 requ√™tes (-60%)

**Pas de tests unitaires s√©par√©s** : Le benchmark = utiliser AETHERFLOW pour construire l'√©tape elle-m√™me.

**Compatibilit√© avec plans Cursor** :
- ‚úÖ **Plan Gratuit** : 50 requ√™tes premium/mois = ~25 t√¢ches AETHERFLOW/mois
- ‚úÖ **Plan Pro** : 500 fast + illimit√© slow = ~250 t√¢ches AETHERFLOW/mois (fast) + illimit√© (slow)
- ‚úÖ **Mode Slow Premium** : Disponible si fast premium √©puis√© (illimit√©, d√©lai 1:30-2:00 min)

---

## Plan d'Impl√©mentation par √âtapes

### √âtape 1 : Int√©grer AgentRouter dans Orchestrator ‚úÖ **TERMIN√â**

**Statut** : ‚úÖ **COMPL√âT√â**

**Ce qui a √©t√© fait** :
- ‚úÖ Orchestrator utilise maintenant `AgentRouter` (ligne 38 de `orchestrator.py`)
- ‚úÖ `agent_router.execute_step()` est appel√© pour chaque √©tape (ligne 188)
- ‚úÖ Support multi-providers activ√© dans Orchestrator

**Fichiers modifi√©s** :
- `Backend/Prod/orchestrator.py` : Utilise `self.agent_router = AgentRouter()`
- `Backend/Prod/models/agent_router.py` : Routage intelligent impl√©ment√©

**R√©sultat** : Les benchmarks peuvent maintenant utiliser Codestral, Gemini et Groq via AgentRouter.

---

### √âtape 2 : Impl√©menter GeminiClient ‚úÖ **TERMIN√â**

**Statut** : ‚úÖ **COMPL√âT√â**

**Ce qui a √©t√© fait** :
- ‚úÖ `Backend/Prod/models/gemini_client.py` cr√©√© et fonctionnel
- ‚úÖ Configuration dans `settings.py` : `gemini_api_key`, `gemini_model`, co√ªts (0.0)
- ‚úÖ Int√©gr√© dans `AgentRouter` avec initialisation automatique
- ‚úÖ Routage intelligent : Gemini s√©lectionn√© pour t√¢ches `analysis`

**Fichiers cr√©√©s/modifi√©s** :
- `Backend/Prod/models/gemini_client.py` : Client Gemini complet
- `Backend/Prod/models/agent_router.py` : Initialisation Gemini ajout√©e
- `Backend/Prod/config/settings.py` : Configuration Gemini ajout√©e

**R√©sultat** : Gemini disponible pour analyse/parsing (gratuit avec quota).

---

### √âtape 3 : Impl√©menter GroqClient ‚úÖ **TERMIN√â**

**Statut** : ‚úÖ **COMPL√âT√â**

**Ce qui a √©t√© fait** :
- ‚úÖ `Backend/Prod/models/groq_client.py` cr√©√© et fonctionnel
- ‚úÖ Configuration dans `settings.py` : `groq_api_key`, `groq_model`, co√ªts (0.00059/0.00079 per 1K)
- ‚úÖ Int√©gr√© dans `AgentRouter` avec initialisation automatique
- ‚úÖ Routage intelligent : Groq s√©lectionn√© pour t√¢ches `prototyping`/`brainstorming`

**Fichiers cr√©√©s/modifi√©s** :
- `Backend/Prod/models/groq_client.py` : Client Groq complet (OpenAI-compatible)
- `Backend/Prod/models/agent_router.py` : Initialisation Groq ajout√©e
- `Backend/Prod/config/settings.py` : Configuration Groq + co√ªts ajout√©s

**R√©sultat** : Groq disponible pour prototypage rapide (ultra-fast).

---

### √âtape 4 : Impl√©menter Routage Intelligent ‚úÖ **TERMIN√â**

**Statut** : ‚úÖ **COMPL√âT√â**

**Ce qui a √©t√© fait** :
- ‚úÖ `select_provider_for_step()` impl√©ment√© dans `agent_router.py`
- ‚úÖ Routage intelligent bas√© sur `step.type`, `complexity`, `estimated_tokens`
- ‚úÖ Logique compl√®te : Gemini (analysis), Codestral (refactoring), DeepSeek (code_generation), Groq (prototyping)
- ‚úÖ M√©thodes utilitaires : `get_available_providers()`, `get_routing_info()`, `_get_routing_reasoning()`

**Logique de routage impl√©ment√©e** :
```python
def select_provider_for_step(self, step: Step, provider: Optional[str] = None) -> BaseLLMClient:
    # Si provider sp√©cifi√©, l'utiliser
    if provider:
        return self._get_client(provider)
    
    # Routage intelligent selon type/complexity/tokens
    if step.type == "analysis":
        return self.gemini_client  # Gratuit
    elif step.type == "refactoring":
        return self.codestral_client  # Pr√©cision
    elif step.type == "code_generation":
        if step.complexity > 0.7 or step.estimated_tokens > 4000:
            return self.deepseek_client  # Qualit√©
        else:
            return self.groq_client  # Rapide
    elif step.type == "prototyping":
        return self.groq_client  # Ultra-fast
    else:
        return self.deepseek_client  # D√©faut
```

**R√©sultats** :
- ‚úÖ Routage automatique fonctionnel
- ‚úÖ √âconomies significatives : Gemini gratuit pour analyses ‚Üí $0.00
- ‚úÖ Benchmark d√©montr√© : Co√ªt r√©duit de ~60% avec routage intelligent

**Fichiers modifi√©s** :
- `Backend/Prod/models/agent_router.py` : Routage intelligent complet impl√©ment√©

---

### √âtape 5.5 : Monitoring Temps R√©el ‚úÖ **TERMIN√â**

**Statut** : ‚úÖ **COMPL√âT√â**

**Ce qui a √©t√© fait** :
- ‚úÖ `Backend/Prod/models/execution_monitor.py` cr√©√© avec `ExecutionMonitor`
- ‚úÖ Affichage temps r√©el avec Rich (tableaux, panneaux)
- ‚úÖ Suivi de chaque √©tape : statut, provider, temps, tokens, co√ªt
- ‚úÖ Int√©gr√© dans `orchestrator.py` avec mise √† jour automatique
- ‚úÖ R√©sum√© global en temps r√©el (progression, co√ªts cumul√©s, temps √©coul√©)

**Fonctionnalit√©s** :
- ‚úÖ Tableau mis √† jour toutes les 2 secondes
- ‚úÖ Statut visuel : ‚úì Completed, ‚ü≥ Running, ‚úó Failed, ‚óã Pending
- ‚úÖ Provider tracking : affichage du provider utilis√© (Gemini, DeepSeek, Codestral, Groq)
- ‚úÖ M√©triques par √©tape : temps d'ex√©cution, tokens, co√ªt
- ‚úÖ R√©sum√© global : progression totale, co√ªts cumul√©s, temps √©coul√©
- ‚úÖ Compatible terminal interactif et non-interactif

**Fichiers cr√©√©s/modifi√©s** :
- `Backend/Prod/models/execution_monitor.py` : Module de monitoring complet
- `Backend/Prod/orchestrator.py` : Int√©gration du monitoring
- `docs/guides/MONITORING_TEMPS_REEL.md` : Documentation compl√®te

**R√©sultat** : Visibilit√© compl√®te de l'ex√©cution en temps r√©el. Plus d'opacit√© pendant l'ex√©cution.

**Documentation** : Voir `/docs/guides/MONITORING_TEMPS_REEL.md` pour les d√©tails.

---

### √âtape 5 : Tracking Temps Claude Code ‚ùå **ANNUL√â**

**Statut** : ‚ùå **ANNUL√â - D√©cision prise le 26 janvier 2025**

**Raison** : M√©trique bas√©e sur des estimations arbitraires, pas de valeur op√©rationnelle r√©elle.

**Analyse compl√®te** : Voir `/docs/guides/ANALYSE_METRIQUE_TEMPS_CLAUDE.md`

**Ce qui a √©t√© fait** :
- ‚úÖ Plan `task_claude_time_tracker.json` cr√©√© et ex√©cut√© via AETHERFLOW (pour benchmark)
- ‚úÖ Code g√©n√©r√© dans `output/claude_time_tracker/step_outputs/` (conserv√© pour r√©f√©rence)
- ‚úÖ Benchmark ex√©cut√© : 100% succ√®s

**D√©cision** : Ne pas int√©grer cette m√©trique. Les m√©triques r√©elles (temps, co√ªt, tokens, succ√®s) sont suffisantes.

---

### √âtape 6 : Am√©liorer Scripts de Benchmarking üü° **EN COURS D'IMPL√âMENTATION**

**Statut** : ‚è≥ **PARTIELLEMENT COMPL√âT√â**
- ‚úÖ M√©triques √©tendues (TTFT, TTR, cache, provider)
- ‚úÖ Script `benchmark_comprehensive.py` cr√©√©
- ‚úÖ Script `run_benchmark_suite.py` cr√©√©
- ‚è≥ G√©n√©ration de graphiques (en attente)
- ‚è≥ Scripts sp√©cifiques par optimisation (en attente)

**Objectif** : Scripts complets qui mesurent toutes les m√©triques r√©elles (co√ªts, temps, qualit√©, latence)

**Fichiers cr√©√©s/modifi√©s** :
- ‚úÖ `scripts/benchmark_comprehensive.py` : Benchmark complet avec toutes les m√©triques (CR√â√â)
- ‚úÖ `scripts/run_benchmark_suite.py` : Suite de benchmarks avec comparaisons (CR√â√â)
- ‚úÖ `Backend/Prod/models/metrics.py` : M√©triques √©tendues avec latence (MODIFI√â)
- ‚úÖ `Backend/Prod/orchestrator.py` : Passage provider aux m√©triques (MODIFI√â)
- ‚úÖ `scripts/README_BENCHMARK.md` : Documentation d'utilisation (CR√â√â)
- ‚è≥ `scripts/benchmark_prompt_cache.py` : Tests sp√©cifiques prompt caching (√Ä FAIRE)
- ‚ùå `scripts/benchmark_slm_local.py` : Tests sp√©cifiques SLM locaux (ANNUL√â - contrainte technique)
- ‚è≥ `scripts/benchmark_speculative.py` : Tests sp√©cifiques speculative decoding (√Ä FAIRE)
- ‚è≥ `scripts/benchmark_semantic_cache.py` : Tests sp√©cifiques cache s√©mantique (√Ä FAIRE)

**M√©triques √† am√©liorer** :
- ‚úÖ Temps d'ex√©cution r√©el (d√©j√† mesur√©)
- ‚úÖ Co√ªt API r√©el (d√©j√† mesur√©)
- ‚úÖ Tokens utilis√©s (d√©j√† mesur√©)
- ‚úÖ Taux de succ√®s (d√©j√† mesur√©)
- ‚úÖ Comparaison providers : DeepSeek vs Gemini vs Codestral vs Groq (IMPL√âMENT√â)
- ‚úÖ Comparaison avant/apr√®s optimisations (IMPL√âMENT√â dans run_benchmark_suite.py)
- ‚è≥ M√©triques de qualit√© : taux de code fonctionnel du premier coup (√Ä FAIRE)
- ‚è≥ **M√©triques de latence** :
  - ‚è≥ TTFT (Time To First Token) - Cible <2s
  - ‚è≥ TTR (Time To Response) - Cible <30s
  - ‚è≥ Queue Latency - Cible <1s
  - ‚è≥ Network Overhead - Cible <500ms
- ‚è≥ **M√©triques de cache** :
  - ‚è≥ Cache Hit Rate (Prompt) - Cible >60%
  - ‚è≥ Cache Hit Rate (S√©mantique) - Cible >40%
  - ‚è≥ Cache Read Cost - Cible 0.1√ó
- ‚è≥ **M√©triques speculative decoding** :
  - ‚è≥ Speculative Accept Rate - Cible >70%
  - ‚è≥ Speedup Factor - Cible >1.5√ó
- ‚ùå **M√©triques SLM locaux** : ANNUL√â (contrainte technique - machine i7 4 c≈ìurs)
  - **Alternative** : Utiliser m√©triques Groq (d√©j√† int√©gr√©, latence 1-3s)

**Rapport** :
- Graphiques : Temps par provider
- Graphiques : Co√ªt par provider
- Graphiques : Comparaison avant/apr√®s parall√©lisation
- ‚úÖ Tableaux comparatifs : Performance de chaque provider (IMPL√âMENT√â)
- ‚úÖ Tableaux comparatifs : Performance par type de t√¢che (IMPL√âMENT√â)
- ‚è≥ Graphiques : TTFT reduction, cache hit rate, speedup factor (√Ä FAIRE)
- ‚è≥ Graphiques : Temps par provider (√Ä FAIRE)
- ‚è≥ Graphiques : Co√ªt par provider (√Ä FAIRE)
- ‚è≥ Tableaux comparatifs : M√©triques avant/apr√®s optimisations latence (Structure pr√™te)

**Ce qui a √©t√© fait** :
- ‚úÖ Extension de `StepMetrics` avec m√©triques de latence (TTFT, TTR, queue latency, network overhead)
- ‚úÖ Extension de `StepMetrics` avec m√©triques de cache (cache_hit, cache_read_cost_multiplier)
- ‚úÖ Ajout du provider dans les m√©triques
- ‚úÖ Script `benchmark_comprehensive.py` avec analyses par provider et par type
- ‚úÖ Script `run_benchmark_suite.py` pour comparaisons multiples
- ‚úÖ Documentation compl√®te dans `scripts/README_BENCHMARK.md`

**Ce qui reste √† faire** :
- ‚è≥ G√©n√©ration de graphiques (matplotlib/plotly)
- ‚è≥ Scripts sp√©cifiques par optimisation (prompt cache, SLM, speculative, semantic cache)
- ‚è≥ Int√©gration m√©triques TTFT/TTR r√©elles (n√©cessite modifications clients)
- ‚è≥ M√©triques de qualit√© (code quality score, first try success rate)

**R√©f√©rence** : Voir `/docs/guides/STRATEGIE_BENCHMARK_LATENCE.md` pour strat√©gie compl√®te

---

### √âtape 7 : Parall√©lisation des T√¢ches Ind√©pendantes ‚úÖ **TERMIN√â**

**Statut** : ‚úÖ **COMPL√âT√â ET TEST√â**

**Objectif** : Ex√©cuter les t√¢ches sans d√©pendances en parall√®le pour r√©duire le temps total d'ex√©cution

**Probl√®me actuel** :
- Ligne 97 de `orchestrator.py` : Commentaire "can be parallelized in future"
- Les √©tapes dans un batch sont ex√©cut√©es s√©quentiellement m√™me si elles sont ind√©pendantes
- Perte de temps : si 3 √©tapes ind√©pendantes prennent chacune 30s, total = 90s au lieu de ~30s

**Solution** :
- Utiliser `asyncio.gather()` pour ex√©cuter les √©tapes d'un batch en parall√®le
- Conserver l'ordre s√©quentiel entre batches (pour respecter les d√©pendances)

**Fichiers √† modifier** :
- `Backend/Prod/orchestrator.py` : Lignes 94-122 (boucle d'ex√©cution des batches)

**Changements √† impl√©menter** :
```python
# Avant (ligne 97-122) : S√©quentiel
for step in batch:
    result = await self._execute_step(step, context, results)
    results[step.id] = result
    self.metrics.record_step_result(step, result)
    # ...

# Apr√®s : Parall√®le pour batch ind√©pendants
if len(batch) > 1:
    # Ex√©cuter toutes les √©tapes du batch en parall√®le
    step_tasks = [
        self._execute_step_with_monitoring(step, context, results) 
        for step in batch
    ]
    step_results = await asyncio.gather(*step_tasks)
    
    # Traiter les r√©sultats
    for step, result in zip(batch, step_results):
        results[step.id] = result
        self.metrics.record_step_result(step, result)
        # ...
else:
    # Une seule √©tape : ex√©cution normale
    step = batch[0]
    result = await self._execute_step_with_monitoring(step, context, results)
    # ...
```

**Points d'attention** :
- ‚úÖ Le monitoring doit fonctionner avec la parall√©lisation (plusieurs √©tapes "Running" simultan√©ment)
- ‚úÖ Gestion des erreurs : si une √©tape √©choue, les autres continuent
- ‚úÖ M√©triques : chaque √©tape doit √™tre track√©e individuellement

**Ce qui a √©t√© fait** :
- ‚úÖ Plan `task_parallelization.json` cr√©√© et ex√©cut√© via AETHERFLOW
- ‚úÖ Code int√©gr√© dans `Backend/Prod/orchestrator.py` :
  - M√©thode `_execute_batch_parallel()` avec `asyncio.gather()`
  - M√©thode `_execute_step_with_monitoring()` pour encapsulation
  - D√©tection automatique des batch avec plusieurs √©tapes
- ‚úÖ Test r√©ussi : 3 √©tapes ind√©pendantes ex√©cut√©es en parall√®le
- ‚úÖ Monitoring fonctionne avec plusieurs √©tapes "Running" simultan√©ment

**R√©sultats du test** :
- **Gain mesur√©** : 44% plus rapide (82s ‚Üí 46s pour 4 √©tapes)
- **Parall√©lisation valid√©e** : 3 √©tapes ex√©cut√©es simultan√©ment (~18s au lieu de ~54s)
- **Taux de succ√®s** : 100% (4/4 √©tapes r√©ussies)

**Fichiers modifi√©s** :
- `Backend/Prod/orchestrator.py` : Parall√©lisation int√©gr√©e (lignes 98-200)

---

### √âtape 8 : RAG PageIndex pour Contexte Projet (Phase 3)

**Statut** : üîµ **PHASE 3 - Apr√®s parall√©lisation**

**Objectif** : Remplacer l'approche vectorielle (ChromaDB) par PageIndex - index hi√©rarchique raisonn√© pour docs structur√©s (PRD, roadmap) et codebase

**Approche PageIndex** :
- **Avantage** : Remplace avantageusement le RAG vectoriel (ChromaDB) par un index hi√©rarchique raisonn√©
- **Id√©al pour** : Docs structur√©s (PRD, roadmap) et future codebase
- **Principe** : Un LLM (ex. Mistral Small, low-cost) parcourt r√©cursivement les fichiers MD/code comme une table des mati√®res (ToC inf√©r√©e), identifie sections pertinentes ("√âtape 7 parall√©lisation dans PLAN_GENERAL_ROADMAP.md > Section √âtape 7"), et retrieve chunks coh√©rents avec refs pr√©cises (node_id: "roadmap.etape7")

**Int√©gration** :
- Hook dans `orchestrator.py` : `process_request()` enrichit le contexte AVANT planification
- Contexte enrichi pour planification/synth√®se sans embeddings co√ªteux
- Tra√ßabilit√© : "voir PRD 2.2.3" avec refs pr√©cises

**Fichiers √† cr√©er** :
- `Backend/Prod/rag/pageindex_store.py` : Module PageIndexRAG avec LlamaIndex
- `Backend/Prod/rag/` : Module RAG (remplace ChromaDB)

**Impl√©mentation** :
```python
# Backend/Prod/rag/pageindex_store.py
from llama_index.core import SimpleDirectoryReader, PageIndexPlanner

class PageIndexRAG:
    def __init__(self, docs_path: str = "docs/guides"):
        self.reader = SimpleDirectoryReader(input_dir=docs_path, required_exts=[".md", ".py"])
        self.nodes = self.reader.load_data()
        self.planner = PageIndexPlanner.from_documents(self.nodes)
    
    async def retrieve(self, query: str, history: list) -> list:
        plan = await self.planner.aretrieve(query, history)
        return [f"{node.metadata['file_name']}:{node.id}" for node in plan.sources]
```

**Int√©gration dans Orchestrator** :
```python
class Orchestrator:
    def __init__(self):
        # ... existing
        self.rag = PageIndexRAG()  # Remplace ChromaDB
    
    async def process_request(self, user_query: str, context: dict):
        # RAG via PageIndex AVANT planification
        rag_context = await self.rag.retrieve(user_query, context.get('history', []))
        plan_prompt = f"Contexte RAG: {rag_context}\nRequ√™te: {user_query}\n..."
```

**Gains vs ChromaDB** :

| Aspect | ChromaDB (ancien) | PageIndex (nouveau) |
|--------|-------------------|---------------------|
| Setup | Embeddings lourds | Z√©ro vector DB, index instantan√© |
| Pr√©cision | Similarit√© lexicale | Raisonnement s√©mantique (98%+ sur docs struct.) |
| Co√ªt | GPU/Stockage | CPU-only (~$0/t√¢che) |
| Tra√ßabilit√© | Chunks anonymes | "Fichier:section.ligne" |
| √âvolutif | R√©-index full | Incr√©mental (add file) |

**M√©triques √† ajouter** :
- `rag_efficiency` : Pr√©cision des r√©f√©rences (target 95%)
- `cache_hit_rate` : Taux de cache (existant)
- `retrieval_time_ms` : Temps de r√©cup√©ration du contexte

**D√©pendances** :
- `pip install llama-index llama-parse` (ou via AgentRouter pour LLM)

**Note** : Cette √©tape est pour Phase 3 selon PRD, apr√®s parall√©lisation. PageIndex est pr√©f√©rable √† ChromaDB pour les docs structur√©s.

---

## Script de Benchmarking Unifi√©

**Fichier** : `scripts/run_benchmark_suite.py` (existant, √† am√©liorer)

**Fonctionnalit√©s** :
1. Ex√©cute les plans `task_X_implementation.json` pour chaque √©tape
2. Mesure toutes les m√©triques (temps, co√ªts, tokens, qualit√©)
3. **Calcule temps Claude Code gagn√©** pour chaque √©tape construite
4. G√©n√®re rapport comparatif avec :
   - Temps r√©el AETHERFLOW vs Temps estim√© manuel
   - Temps gagn√© total cumul√©
   - ROI (valeur temps / co√ªt API)
   - Graphiques de comparaison

**M√©triques calcul√©es par √©tape** :
```python
metrics = {
    "aetherflow_time_ms": 120000,  # 2 minutes
    "estimated_manual_time_ms": 1800000,  # 30 minutes estim√©
    "claude_code_time_saved_ms": 1680000,  # 28 minutes gagn√©
    "efficiency_ratio": 15.0,  # 15x plus rapide
    "roi_claude_code": 1400.0,  # $14 √©conomis√© si dev = $50/h
    "cost_api_usd": 0.01,
    "net_savings_usd": 13.99  # √âconomie nette
}
```

---

## Ordre d'Ex√©cution Recommand√©

### ‚úÖ √âtapes Termin√©es (Phase 1 & 2)

1. ‚úÖ **√âtape 1** : Int√©grer AgentRouter dans Orchestrator - **TERMIN√â**
2. ‚úÖ **√âtape 2** : Impl√©menter GeminiClient - **TERMIN√â**
3. ‚úÖ **√âtape 3** : Impl√©menter GroqClient - **TERMIN√â**
4. ‚úÖ **√âtape 4** : Routage Intelligent - **TERMIN√â**
5. ‚úÖ **√âtape 5.5** : Monitoring Temps R√©el - **TERMIN√â**

### ‚è≥ √âtapes Restantes (Priorit√©)

#### üî¥ Priorit√© Haute

1. **√âtape 7** : Parall√©lisation des T√¢ches Ind√©pendantes üî¥ **√Ä FAIRE MAINTENANT**
   - **Objectif** : Ex√©cuter les √©tapes sans d√©pendances en parall√®le
   - **Gain attendu** : ~3x plus rapide pour les batch avec plusieurs √©tapes ind√©pendantes
   - **√Ä FAIRE** : Claude Code g√©n√®re `task_parallelization.json`
   - **√Ä FAIRE** : AETHERFLOW ex√©cute ‚Üí modifie `orchestrator.py` avec `asyncio.gather()`
   - **√Ä FAIRE** : Adapter le monitoring pour g√©rer plusieurs √©tapes "Running" simultan√©ment
   - **√Ä FAIRE** : Tester et benchmarker le gain de temps
   - **Fichier √† modifier** : `Backend/Prod/orchestrator.py` (lignes 94-122)

#### üü° Priorit√© Moyenne

2. **√âtape 6** : Am√©liorer Scripts de Benchmarking
   - **Objectif** : Scripts complets avec mesure temps Claude Code gagn√©
   - **√Ä FAIRE** : Int√©grer m√©triques temps Claude Code dans `run_benchmark_suite.py`
   - **√Ä FAIRE** : Consolidation de tous les rapports de benchmark
   - **√Ä FAIRE** : G√©n√©ration d'un rapport global avec temps Claude Code gagn√© cumul√©
   - **Fichiers √† cr√©er/modifier** :
     - `scripts/benchmark_with_claude_time.py`
     - `scripts/run_benchmark_suite.py` (am√©liorer)

#### üîµ Priorit√© Basse / Phase 3

4. **√âtape 8** : RAG pour Contexte Projet (Phase 3 - diff√©r√©)
   - **Objectif** : Base de connaissances pour contexte projet
   - **Statut** : Peut √™tre report√© √† Phase 3 selon PRD
   - **Fichiers √† cr√©er** :
     - `Backend/Prod/rag/` : Module RAG avec ChromaDB
     - `Backend/Prod/rag/chroma_store.py`
     - `Backend/Prod/rag/indexer.py`

---

## Template de Construction par √âtape

Chaque √©tape suit ce workflow :

1. **Claude Code g√©n√®re le plan** :
   - Cr√©er `Backend/Notebooks/benchmark_tasks/task_X_implementation.json`
   - D√©crire les √©tapes pour construire l'√©tape X
   - Inclure contexte, fichiers de r√©f√©rence, crit√®res de validation

2. **AETHERFLOW ex√©cute le plan** :
   ```bash
   python -m Backend.Prod --plan Backend/Notebooks/benchmark_tasks/task_X_implementation.json --output output/step_X
   ```

3. **Mesure automatique** :
   - Temps d'ex√©cution AETHERFLOW
   - Co√ªt API utilis√©
   - Tokens consomm√©s
   - Qualit√© du code g√©n√©r√© (test fonctionnel)

4. **Calcul temps Claude Code gagn√©** :
   - Temps estim√© manuel (bas√© sur complexit√©)
   - Temps r√©el AETHERFLOW
   - Diff√©rence = temps gagn√©

5. **Rapport g√©n√©r√©** :
   - `output/step_X/benchmark_report.md`
   - Inclut toutes les m√©triques + temps Claude Code gagn√©

---

## Crit√®res de Validation par √âtape

Chaque √©tape doit valider :
- ‚úÖ **Plan g√©n√©r√©** : `task_X_implementation.json` cr√©√© par Claude Code
- ‚úÖ **Construction via AETHERFLOW** : Plan ex√©cut√© avec succ√®s
- ‚úÖ **Code fonctionnel** : Le code g√©n√©r√© fonctionne (test manuel rapide)
- ‚úÖ **Rapport g√©n√©r√©** : M√©triques compl√®tes dans `output/step_X/benchmark_report.md`
- ‚úÖ **Temps Claude Code gagn√©** : Mesur√© et document√© dans le rapport
- ‚úÖ **Pas de r√©gression** : Les fonctionnalit√©s existantes continuent de fonctionner

---

## Documentation Requise

Pour chaque √©tape :
1. **Plan JSON** : `Backend/Notebooks/benchmark_tasks/task_X_implementation.json` g√©n√©r√© par Claude Code
2. **Rapport de benchmark** : `output/step_X/benchmark_report.md` g√©n√©r√© automatiquement par AETHERFLOW
3. **Mise √† jour CONTEXTE.md** : √âtat actuel du projet apr√®s chaque √©tape
4. **Rapport global** : Consolidation de tous les benchmarks avec temps Claude Code gagn√© cumul√©

---

## Utilisation avec Claude Code (Moi) - Guide Complet

### Comment Utiliser AETHERFLOW avec Moi

**Workflow** :
```
Vous (dans Cursor) ‚Üí Moi (Claude Code) ‚Üí G√©n√®re plan.json ‚Üí 
AETHERFLOW ex√©cute (routage intelligent) ‚Üí Code g√©n√©r√© ‚Üí 
Moi v√©rifie ‚Üí Vous recevez le code final
```

### √âconomies R√©alis√©es

**Comparaison : Claude Code Seul vs AETHERFLOW**

| M√©trique | Claude Code Seul | AETHERFLOW + Claude Code | √âconomie |
|----------|------------------|-------------------------|----------|
| **Tokens Claude Code** | ~13,800 | ~2,300 | **-83%** ‚¨áÔ∏è |
| **Utilisations fast premium** | 5 | 2 | **-60%** ‚¨áÔ∏è |
| **Co√ªt API** | $0.00 | $0.0008 | +$0.0008 |
| **Temps total** | ~10-15 min | ~4-5 min | **-50%** ‚¨áÔ∏è |
| **Qualit√©** | Variable | Constante (routage intelligent) | ‚úÖ |

**Exemple concret** : 20 t√¢ches/mois
- **Sans AETHERFLOW** : 100 utilisations fast premium (20% de vos 500/mois)
- **Avec AETHERFLOW** : 40 utilisations fast premium (8% de vos 500/mois)
- **√âconomie** : 60 utilisations (12% √©conomis√©)
- **Co√ªt API AETHERFLOW** : $0.016/mois (n√©gligeable)

### Compatibilit√© avec Plans Cursor

**Plan Gratuit** :
- ‚úÖ 50 requ√™tes premium/mois = ~25 t√¢ches AETHERFLOW/mois
- ‚úÖ Apr√®s √©puisement : Mod√®les gratuits disponibles (illimit√©)
- ‚úÖ AETHERFLOW fonctionne ind√©pendamment

**Plan Pro** :
- ‚úÖ 500 fast + illimit√© slow = ~250 t√¢ches AETHERFLOW/mois (fast)
- ‚úÖ Mode slow premium disponible si besoin (illimit√©, d√©lai 1:30-2:00 min)
- ‚úÖ AETHERFLOW fonctionne toujours

**Mode Slow Premium** :
- ‚úÖ Disponible si fast premium √©puis√© (illimit√©)
- ‚úÖ D√©lai : 1:18 √† 2:00 minutes avant r√©ponse
- ‚úÖ M√™me qualit√©, juste plus lent
- ‚úÖ Impact sur AETHERFLOW : +3 minutes d'attente par t√¢che

### Clarifications Importantes

1. **Claude Code vs Claude API** :
   - **Claude Code (Moi)** = G√©n√®re plans, v√©rifie r√©sultats ‚Üí Toujours disponible dans Cursor
   - **Claude API** = Service externe ‚Üí NON utilis√© dans AETHERFLOW
   - AETHERFLOW utilise DeepSeek/Gemini/Codestral/Groq (ind√©pendant de Claude API)

2. **Abonnements Ind√©pendants** :
   - Abonnement Claude Code personnel ‚â† Abonnement Cursor
   - Les limites sont s√©par√©es et ind√©pendantes
   - Vous pouvez utiliser Cursor m√™me si votre abonnement Claude Code personnel est √©puis√©

3. **Routage Intelligent** :
   - S'applique automatiquement lors de l'ex√©cution AETHERFLOW
   - Gemini pour analyses (gratuit)
   - Codestral pour refactoring (pr√©cision)
   - DeepSeek pour code complexe (qualit√©)
   - Groq pour prototypage (rapide)

**Documentation compl√®te** : Voir `/docs/guides/RAPPORT_CLAUDE_CURSOR_AETHERFLOW.md` pour tous les d√©tails.

---

## üìã R√©capitulatif des T√¢ches Restantes

### üî¥ Priorit√© Haute (√Ä faire en premier)

#### 1. Int√©grer Tracking Temps Claude Code (√âtape 5)

**Statut** : Code g√©n√©r√© par AETHERFLOW, int√©gration en attente

**Actions √† faire** :
1. **Cr√©er** `Backend/Prod/models/claude_time_tracker.py`
   - Source : `output/claude_time_tracker/step_outputs/step_2.txt`
   - **Note** : Adapter Pydantic ‚Üí dataclasses si n√©cessaire

2. **Modifier** `Backend/Prod/models/metrics.py`
   - Ajouter `estimated_manual_time_ms` et `claude_code_time_saved_ms` √† `StepMetrics`
   - Ajouter m√©triques agr√©g√©es √† `PlanMetrics` (total_estimated_manual_time_ms, total_claude_code_time_saved_ms, efficiency_ratio)
   - Int√©grer `ClaudeTimeTracker` dans `MetricsCollector`
   - Mettre √† jour `print_summary()` pour afficher les nouvelles m√©triques
   - Sources : `step_3.txt`, `step_4.txt`, `step_5.txt`, `step_6.txt`
   - **Note** : Adapter Pydantic ‚Üí dataclasses

3. **Cr√©er** `scripts/test_claude_time_tracker.py`
   - Source : `output/claude_time_tracker/step_outputs/step_7.txt`

4. **Tester** l'int√©gration compl√®te

5. **Documenter** les nouvelles m√©triques

**Fichiers sources** : `output/claude_time_tracker/step_outputs/`

---

### üü° Priorit√© Moyenne (√Ä faire ensuite)

#### 2. Parall√©lisation des T√¢ches Ind√©pendantes (√âtape 7)

**Objectif** : Ex√©cuter les √©tapes sans d√©pendances en parall√®le pour r√©duire le temps total

**Actions √† faire** :
1. **Cr√©er** le plan `task_parallelization.json` (via Claude Code)
2. **Ex√©cuter** le plan via AETHERFLOW
3. **Modifier** `Backend/Prod/orchestrator.py` ligne 89 :
   - Remplacer la boucle s√©quentielle par `asyncio.gather()` pour les batch ind√©pendants
4. **Tester** la parall√©lisation
5. **Benchmark** : Mesurer le gain de temps avec parall√©lisation

**Fichier √† modifier** : `Backend/Prod/orchestrator.py`

---

#### 3. Am√©liorer Scripts de Benchmarking (√âtape 6)

**Objectif** : Scripts complets avec mesure temps Claude Code gagn√©

**Actions √† faire** :
1. **Cr√©er** `scripts/benchmark_with_claude_time.py`
   - Int√©grer les m√©triques temps Claude Code
   - Calculer ROI (valeur temps / co√ªt API)

2. **Am√©liorer** `scripts/run_benchmark_suite.py`
   - Ajouter section "Temps Claude Code Gagn√©"
   - G√©n√©rer graphiques : Temps manuel estim√© vs Temps AETHERFLOW
   - Calculer √©conomies cumul√©es

3. **Consolider** tous les rapports de benchmark en un rapport global

**Fichiers √† cr√©er/modifier** :
- `scripts/benchmark_with_claude_time.py` (nouveau)
- `scripts/run_benchmark_suite.py` (modifier)

---

### üîµ Phase 3 : Optimisation (Futur)

#### 4. R√©duction Latence API (√âtape 9)

**Statut** : ‚úÖ **TERMIN√â**

**Objectif** : R√©duire la latence per√ßue via plusieurs techniques d'optimisation

**Impl√©mentations r√©alis√©es** :

‚úÖ **Prompt Caching**
- Prompt caching activ√© pour flows r√©utilisables (system + docs)
- Cache hit rate mesur√©
- R√©duction TTFT : 30-60%

‚ùå **SLM Locaux** : **ANNUL√â - Contrainte technique**
- **Raison** : Machine i7 4 c≈ìurs (insuffisant pour SLM locaux)
- **Alternative** : Utiliser Groq comme "SLM rapide" (1-3s, d√©j√† int√©gr√©)

‚úÖ **Speculative Decoding**
- Impl√©mentation draft + verify avec Groq/Gemini Flash comme draft
- R√©duction TTFT mesur√©e

‚úÖ **Cache S√©mantique**
- Cache bas√© sur embeddings (sentence-transformers)
- Similarit√© cosinus pour r√©utilisation de r√©ponses similaires

‚úÖ **Connection Pooling**
- Utilisation de httpx.AsyncClient pour pooling automatique
- R√©duction overhead r√©seau

**R√©f√©rence** : Voir `ETAPE_9_REDUCTION_LATENCE.md` pour documentation compl√®te

#### 5. Optimisations "Z√©ro Budget" (√âtape 10)

**Statut** : ‚úÖ **TERMIN√â**

**Objectif** : R√©duire la latence de 93% (temps API) via parall√©lisation asynchrone, pipelining, modes d'ex√©cution optimis√©s et optimisation des prompts, sans budget suppl√©mentaire.

**Impl√©mentations r√©alis√©es** :

‚úÖ **Streaming Pipelining**
- `PlanReader.read_streaming()` : Parse JSON au fur et √† mesure
- Ex√©cution commence d√®s que la premi√®re √©tape sans d√©pendances est disponible
- Gain attendu : 20-30% r√©duction temps total

‚úÖ **Modes d'Ex√©cution (FAST/BUILD/DOUBLE-CHECK)**
- `ExecutionRouter` : Routage selon mode d'ex√©cution
- Mode FAST : Groq/Gemini Flash (50-70% plus rapide)
- Mode BUILD : DeepSeek-V3 + Codestral (√©quilibr√©)
- Mode DOUBLE-CHECK : DeepSeek-V3 + Gemini Flash audit (fiabilit√© max)

‚úÖ **Parall√©lisation Massive Am√©lior√©e**
- Tri par priorit√© (complexity, tokens)
- Maximisation parall√©lisme inter-providers
- Rate limiting par provider
- Gain : 5 fichiers ind√©pendants : 150s ‚Üí 35-40s

‚úÖ **Prompt Stripping**
- Templates minimaux par type de t√¢che
- Suppression verbosit√©, exemples redondants
- Gain : 20-30% r√©duction tokens g√©n√©r√©s

‚úÖ **Output Constraints**
- Contraintes de sortie (Code only, JSON only, No prose)
- Impl√©mentation par provider (Gemini: response_mime_type, autres: instructions)
- Gain : 10-15% r√©duction suppl√©mentaire tokens

**R√©f√©rence** : Voir `OPTIMISATIONS_ZERO_BUDGET.md` pour documentation compl√®te
- **Note** : Groq offre latence √©quivalente sans ressources locales

**Semaine 3-6 : Speculative Decoding**
- Ajouter draft + verify pipeline pour t√¢ches longues/critiques
- Mesurer speculative accept rate (cible >70%)
- Speedup factor cible : >1.5√ó
- Ajuster draft model size selon r√©sultats

**Continu : Cache S√©mantique + WebSockets**
- Cache s√©mantique local (Redis + Vector DB)
- Connexions persistantes pour sessions longues
- R√©duction overhead r√©seau : >30%

**M√©triques √† tracker** :
- TTFT (Time To First Token) - Cible <2s
- TTR (Time To Response) - Cible <30s
- Cache Hit Rate (Prompt) - Cible >60%
- Cache Hit Rate (S√©mantique) - Cible >40%
- Speculative Accept Rate - Cible >70%
- SLM Call Rate - Cible >30%
- Network Calls Saved - Cible >20%

**Fichiers √† cr√©er** :
- `Backend/Prod/cache/prompt_cache.py` : Module prompt caching
- ‚ùå `Backend/Prod/slm/local_slm.py` : Module SLM locaux (ANNUL√â - contrainte technique)
- `Backend/Prod/speculative/decoder.py` : Module speculative decoding
- `Backend/Prod/cache/semantic_cache.py` : Module cache s√©mantique

**R√©f√©rences** :
- `/docs/guides/Plan de r√©dcution de la latence API.md` : Plan d√©taill√©
- `/docs/guides/STRATEGIE_BENCHMARK_LATENCE.md` : Strat√©gie de benchmark

---

#### 5. RAG pour Contexte Projet (√âtape 8)

**Statut** : Peut √™tre report√© √† Phase 3 selon PRD

**Actions √† faire** (quand Phase 3 d√©marrera) :
1. Cr√©er module RAG avec PageIndex (LlamaIndex) - remplace ChromaDB
2. Impl√©menter `PageIndexRAG` avec `PageIndexPlanner`
3. Int√©grer dans Orchestrator pour contexte enrichi AVANT planification
4. Tester sur PRD/ROADMAP pour benchmark pr√©cision (target 95%)

**Fichiers √† cr√©er** :
- `Backend/Prod/rag/` (dossier)
- `Backend/Prod/rag/pageindex_store.py` : Module PageIndexRAG

**Avantages PageIndex vs ChromaDB** :
- ‚úÖ Z√©ro vector DB, index instantan√©
- ‚úÖ Raisonnement s√©mantique (98%+ pr√©cision sur docs structur√©s)
- ‚úÖ CPU-only (~$0/t√¢che)
- ‚úÖ Tra√ßabilit√© pr√©cise : "Fichier:section.ligne"
- ‚úÖ √âvolutif : incr√©mental (add file)

**R√©f√©rence** : Voir `/docs/guides/Nouveau- Backend-Prod-rag-pageindex_store.py.md` pour d√©tails complets

---

## üéØ Ordre d'Ex√©cution Recommand√©

### Court Terme (Maintenant)
1. ‚úÖ **Parall√©lisation (√âtape 7)** : TERMIN√â ‚úÖ
2. ‚è≥ **Am√©liorer scripts benchmarking (√âtape 6)** : EN COURS üü°
   - Ajouter m√©triques de latence (TTFT, TTR, cache hit rate)
   - Comparaisons avant/apr√®s optimisations
   - Scripts sp√©cifiques par optimisation

### Moyen Terme (Phase 2 - Suite)
3. üîÆ **RAG PageIndex (√âtape 8)** : Si n√©cessaire en Phase 3 üîµ

### Long Terme (Phase 3 - Optimisation)
4. ‚úÖ **R√©duction latence API (√âtape 9)** : TERMIN√â
   - ‚úÖ Prompt caching
   - ‚ùå SLM locaux (ANNUL√â - contrainte technique i7 4 c≈ìurs)
   - ‚úÖ Speculative decoding
   - ‚úÖ Cache s√©mantique
   - ‚úÖ Connection pooling
   - **R√©f√©rence** : Voir `ETAPE_9_REDUCTION_LATENCE.md`

5. ‚úÖ **Optimisations "Z√©ro Budget" (√âtape 10)** : TERMIN√â
   - ‚úÖ Streaming pipelining (ex√©cution d√®s que premi√®re √©tape disponible)
   - ‚úÖ Modes d'ex√©cution (FAST/BUILD/DOUBLE-CHECK)
   - ‚úÖ Parall√©lisation massive am√©lior√©e
   - ‚úÖ Prompt stripping (r√©duction 20-30% tokens)
   - ‚úÖ Output constraints (r√©duction suppl√©mentaire 10-15% tokens)
   - **R√©f√©rence** : Voir `OPTIMISATIONS_ZERO_BUDGET.md`

---

**Derni√®re mise √† jour** : 26 janvier 2025
