# Strat√©gie de Benchmark pour R√©duction Latence API

**Date** : 26 janvier 2025  
**Contexte** : Plan de r√©duction de la latence API n√©cessite une strat√©gie de benchmark adapt√©e

---

## üìä Analyse du Plan de R√©duction de Latence

### Techniques Propos√©es (par ordre de priorit√©)

1. **Prompt Caching** (Semaine 1-2)
   - R√©duction TTFT estim√©e : **30-60%**
   - Cache hit rate cible : **>60%**
   - Co√ªt : Cache reads = 0.1√ó prix input

2. **SLM Locaux** (Semaine 2-4)
   - R√©duction appels externes : **20-40%**
   - Utilisation : Validation, formatage, linting

3. **Speculative Decoding** (Semaine 3-6)
   - Draft + Verify pattern
   - Mesure : Speculative accept rate

4. **Cache S√©mantique Local** (Continu)
   - Redis + Vector DB
   - D√©duplication r√©ponses similaires

5. **WebSockets/Connexions Persistantes** (Continu)
   - R√©duction overhead r√©seau

---

## üéØ M√©triques √† Benchmarker (Nouvelles)

### M√©triques Existantes (D√©j√† Mesur√©es)
- ‚úÖ Temps d'ex√©cution total
- ‚úÖ Co√ªt API
- ‚úÖ Tokens utilis√©s
- ‚úÖ Taux de succ√®s

### M√©triques Nouvelles √† Ajouter

#### 1. M√©triques de Latence Granulaire

| M√©trique | Description | Cible | Priorit√© |
|----------|-------------|-------|----------|
| **TTFT (Time To First Token)** | Temps avant premier token | <2s | üî¥ Critique |
| **TTR (Time To Response)** | Temps total de r√©ponse | <30s | üî¥ Critique |
| **Queue Latency** | Temps d'attente en file | <1s | üü° Moyenne |
| **Network Overhead** | Temps DNS + TCP + TLS | <500ms | üü° Moyenne |

#### 2. M√©triques de Cache

| M√©trique | Description | Cible | Priorit√© |
|----------|-------------|-------|----------|
| **Cache Hit Rate (Prompt)** | % requ√™tes utilisant cache | >60% | üî¥ Critique |
| **Cache Hit Rate (S√©mantique)** | % r√©ponses d√©dupliqu√©es | >40% | üü° Moyenne |
| **Cache Read Cost** | Co√ªt cache vs full generation | 0.1√ó | üî¥ Critique |
| **Cache TTL Effectiveness** | Dur√©e optimale du cache | 5min-1h | üü° Moyenne |

#### 3. M√©triques Speculative Decoding

| M√©trique | Description | Cible | Priorit√© |
|----------|-------------|-------|----------|
| **Speculative Accept Rate** | % tokens/branches accept√©s | >70% | üî¥ Critique |
| **Draft Model Speed** | Temps g√©n√©ration draft | <5s | üü° Moyenne |
| **Verify Model Speed** | Temps v√©rification | <10s | üü° Moyenne |
| **Speedup Factor** | Gain vs g√©n√©ration normale | >1.5√ó | üî¥ Critique |

#### 4. M√©triques SLM Locaux

| M√©trique | Description | Cible | Priorit√© |
|----------|-------------|-------|----------|
| **SLM Call Rate** | % appels rout√©s localement | >30% | üü° Moyenne |
| **SLM Latency** | Temps r√©ponse SLM local | <1s | üü° Moyenne |
| **Network Calls Saved** | Nombre appels √©vit√©s | >20% | üü° Moyenne |
| **SLM Accuracy** | Taux succ√®s vs cloud | >95% | üî¥ Critique |

---

## üìã Suite au Plan : Prospective de Tests

### Phase 1 : Baseline (Avant Optimisations)

**Objectif** : √âtablir les m√©triques de r√©f√©rence

**Tests √† Effectuer** :
1. **Benchmark Baseline** :
   - Ex√©cuter 10 plans repr√©sentatifs
   - Mesurer : TTFT, TTR, co√ªt, tokens, succ√®s
   - Documenter : Temps moyen par provider, par type de t√¢che

2. **Analyse des Goulots d'√âtranglement** :
   - Identifier o√π la latence est la plus √©lev√©e
   - Mesurer : Network overhead, queue latency, TTFT par provider
   - Documenter : Distribution des temps (p50, p95, p99)

**Livrables** :
- Rapport baseline avec m√©triques compl√®tes
- Graphiques : Distribution TTFT, TTR par provider
- Identification des 3 principaux goulots d'√©tranglement

---

### Phase 2 : Prompt Caching (Semaine 1-2)

**Objectif** : Mesurer l'impact du prompt caching

**Tests √† Effectuer** :
1. **Test Cache Hit** :
   - Ex√©cuter 20 plans avec contexte r√©utilisable
   - Mesurer : TTFT avant/apr√®s cache, cache hit rate
   - Comparer : Co√ªt avec/sans cache

2. **Test Cache Breakpoints** :
   - Varier les breakpoints de cache
   - Mesurer : Impact sur TTFT et co√ªt
   - Identifier : Breakpoints optimaux

3. **Test Cache TTL** :
   - Tester TTL 5min vs 1h
   - Mesurer : Cache hit rate, co√ªt, coh√©rence
   - Identifier : TTL optimal par type de contexte

**M√©triques Cl√©s** :
- **TTFT Reduction** : Cible 30-60%
- **Cache Hit Rate** : Cible >60%
- **Cost Reduction** : Cible 40-50% (cache reads = 0.1√ó)

**Livrables** :
- Rapport comparaison avant/apr√®s prompt caching
- Graphiques : TTFT reduction, cache hit rate over time
- Recommandations : Breakpoints et TTL optimaux

---

### Phase 3 : SLM Locaux (Semaine 2-4)

**Objectif** : Mesurer l'impact des SLM locaux

**Tests √† Effectuer** :
1. **Test Routage SLM** :
   - Router 30% des appels vers SLM local
   - Mesurer : Latence SLM vs cloud, taux de succ√®s
   - Comparer : Qualit√© des r√©ponses

2. **Test Types de T√¢ches** :
   - Identifier quelles t√¢ches peuvent √™tre rout√©es localement
   - Mesurer : Validation, formatage, linting
   - Documenter : Taux de succ√®s par type

3. **Test Charge** :
   - Tester avec charge √©lev√©e (10+ requ√™tes parall√®les)
   - Mesurer : Latence SLM sous charge
   - Comparer : SLM vs cloud sous charge

**M√©triques Cl√©s** :
- **SLM Call Rate** : Cible >30%
- **Network Calls Saved** : Cible >20%
- **SLM Accuracy** : Cible >95%

**Livrables** :
- Rapport comparaison SLM vs cloud
- Graphiques : Latence SLM, network calls saved
- Recommandations : Types de t√¢ches √† router localement

---

### Phase 4 : Speculative Decoding (Semaine 3-6)

**Objectif** : Mesurer l'impact du speculative decoding

**Tests √† Effectuer** :
1. **Test Draft + Verify** :
   - Ex√©cuter plans avec speculative decoding
   - Mesurer : Speculative accept rate, speedup factor
   - Comparer : Temps total vs g√©n√©ration normale

2. **Test Mod√®les Draft** :
   - Tester diff√©rents mod√®les draft (Qwen, Phi-4, Groq)
   - Mesurer : Accept rate, vitesse, co√ªt
   - Identifier : Mod√®le draft optimal

3. **Test Branches Sp√©culatives** :
   - Tester 2-3 branches parall√®les
   - Mesurer : Temps total, co√ªt, qualit√© r√©sultat final
   - Comparer : Branches vs g√©n√©ration s√©quentielle

**M√©triques Cl√©s** :
- **Speculative Accept Rate** : Cible >70%
- **Speedup Factor** : Cible >1.5√ó
- **Cost Efficiency** : Co√ªt total acceptable vs gain temps

**Livrables** :
- Rapport comparaison speculative vs normal
- Graphiques : Accept rate, speedup factor
- Recommandations : Mod√®les et strat√©gies optimales

---

### Phase 5 : Cache S√©mantique (Continu)

**Objectif** : Mesurer l'impact du cache s√©mantique

**Tests √† Effectuer** :
1. **Test Similarit√©** :
   - Tester d√©duplication r√©ponses similaires
   - Mesurer : Cache hit rate s√©mantique, qualit√© r√©ponses
   - Comparer : R√©ponses d√©dupliqu√©es vs g√©n√©r√©es

2. **Test Vector DB** :
   - Tester diff√©rents Vector DB (Milvus, Pinecone, Weaviate)
   - Mesurer : Latence recherche, pr√©cision, co√ªt
   - Identifier : Solution optimale

**M√©triques Cl√©s** :
- **Cache Hit Rate (S√©mantique)** : Cible >40%
- **Search Latency** : Cible <100ms
- **Similarity Threshold** : Identifier seuil optimal

---

### Phase 6 : WebSockets (Continu)

**Objectif** : Mesurer l'impact des connexions persistantes

**Tests √† Effectuer** :
1. **Test Sessions Longues** :
   - Tester sessions avec WebSockets
   - Mesurer : Network overhead, TTFT am√©lior√©
   - Comparer : WebSocket vs HTTP pour sessions longues

**M√©triques Cl√©s** :
- **Network Overhead Reduction** : Cible >30%
- **TTFT Improvement** : Cible >20%

---

## üîß Am√©liorations N√©cessaires aux Scripts de Benchmark

### 1. Ajouter M√©triques Granulaires

**Fichier** : `scripts/benchmark.py`

**Modifications** :
```python
# Ajouter mesure TTFT, TTR, queue latency
metrics = {
    "ttft_ms": [],  # Time to first token
    "ttr_ms": [],   # Time to response
    "queue_latency_ms": [],  # Queue wait time
    "network_overhead_ms": [],  # DNS + TCP + TLS
    "cache_hits": 0,  # Prompt cache hits
    "cache_misses": 0,
    "slm_calls": 0,  # SLM local calls
    "speculative_accept_rate": 0.0,  # Speculative decoding
}
```

### 2. Ajouter Comparaisons Avant/Apr√®s

**Fichier** : `scripts/run_benchmark_suite.py`

**Modifications** :
- Comparer m√©triques avant/apr√®s chaque optimisation
- G√©n√©rer graphiques : TTFT reduction, cache hit rate, speedup factor
- Tableaux comparatifs : Performance par phase d'optimisation

### 3. Ajouter Tests Sp√©cifiques par Optimisation

**Nouveaux Scripts** :
- `scripts/benchmark_prompt_cache.py` : Tests sp√©cifiques prompt caching
- `scripts/benchmark_slm_local.py` : Tests sp√©cifiques SLM locaux
- `scripts/benchmark_speculative.py` : Tests sp√©cifiques speculative decoding
- `scripts/benchmark_semantic_cache.py` : Tests sp√©cifiques cache s√©mantique

### 4. Ajouter M√©triques de Qualit√©

**Nouvelles M√©triques** :
- **Code Quality Score** : Score qualit√© code g√©n√©r√© (linting, tests)
- **First Try Success Rate** : % code fonctionnel du premier coup
- **Correction Rate** : Nombre corrections n√©cessaires

---

## üìä Plan de Benchmark Complet

### Structure des Tests

```
Phase 0: Baseline (Semaine 0)
‚îú‚îÄ Benchmark 10 plans repr√©sentatifs
‚îú‚îÄ Mesurer toutes m√©triques de base
‚îî‚îÄ Identifier goulots d'√©tranglement

Phase 1: Prompt Caching (Semaine 1-2)
‚îú‚îÄ Test cache hit rate
‚îú‚îÄ Test cache breakpoints
‚îú‚îÄ Test cache TTL
‚îî‚îÄ Comparaison avant/apr√®s

Phase 2: SLM Locaux (Semaine 2-4)
‚îú‚îÄ Test routage SLM
‚îú‚îÄ Test types de t√¢ches
‚îú‚îÄ Test charge
‚îî‚îÄ Comparaison SLM vs cloud

Phase 3: Speculative Decoding (Semaine 3-6)
‚îú‚îÄ Test draft + verify
‚îú‚îÄ Test mod√®les draft
‚îú‚îÄ Test branches sp√©culatives
‚îî‚îÄ Comparaison speculative vs normal

Phase 4: Cache S√©mantique (Continu)
‚îú‚îÄ Test similarit√©
‚îú‚îÄ Test Vector DB
‚îî‚îÄ Comparaison cache s√©mantique vs g√©n√©ration

Phase 5: WebSockets (Continu)
‚îú‚îÄ Test sessions longues
‚îî‚îÄ Comparaison WebSocket vs HTTP
```

### Fr√©quence des Benchmarks

- **Baseline** : 1 fois (avant optimisations)
- **Chaque Optimisation** : Avant/apr√®s + suivi continu
- **Rapport Global** : Mensuel avec toutes les m√©triques

---

## üéØ Crit√®res de Succ√®s par Phase

### Phase 1 : Prompt Caching
- ‚úÖ TTFT r√©duit de **30-60%**
- ‚úÖ Cache hit rate **>60%**
- ‚úÖ Co√ªt r√©duit de **40-50%**

### Phase 2 : SLM Locaux
- ‚úÖ **>30%** appels rout√©s localement
- ‚úÖ **>20%** appels r√©seau √©vit√©s
- ‚úÖ **>95%** pr√©cision SLM

### Phase 3 : Speculative Decoding
- ‚úÖ Accept rate **>70%**
- ‚úÖ Speedup factor **>1.5√ó**
- ‚úÖ Co√ªt total acceptable

### Phase 4 : Cache S√©mantique
- ‚úÖ Cache hit rate **>40%**
- ‚úÖ Search latency **<100ms**

### Phase 5 : WebSockets
- ‚úÖ Network overhead r√©duit de **>30%**
- ‚úÖ TTFT am√©lior√© de **>20%**

---

## üìà Rapports √† G√©n√©rer

### 1. Rapport Baseline
- M√©triques compl√®tes avant optimisations
- Identification goulots d'√©tranglement
- Graphiques distribution temps

### 2. Rapport par Optimisation
- Comparaison avant/apr√®s
- M√©triques sp√©cifiques (cache hit rate, accept rate, etc.)
- Recommandations

### 3. Rapport Global Mensuel
- √âvolution toutes m√©triques
- Comparaison toutes phases
- ROI calcul√© (temps gagn√© / co√ªt)

---

## üöÄ Actions Imm√©diates

### Court Terme (Semaine 1)
1. **Instrumenter m√©triques** : TTFT, TTR, queue latency
2. **Cr√©er baseline** : Ex√©cuter 10 plans repr√©sentatifs
3. **Identifier goulots** : Analyser o√π la latence est la plus √©lev√©e

### Moyen Terme (Semaine 2-4)
1. **Ajouter tests prompt caching** : Scripts sp√©cifiques
2. **Ajouter tests SLM** : Scripts sp√©cifiques
3. **G√©n√©rer rapports** : Comparaisons avant/apr√®s

### Long Terme (Semaine 3-6)
1. **Ajouter tests speculative** : Scripts sp√©cifiques
2. **Ajouter tests cache s√©mantique** : Scripts sp√©cifiques
3. **Automatiser** : Benchmarks continus avec rapports automatiques

---

**Derni√®re mise √† jour** : 26 janvier 2025
